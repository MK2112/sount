{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing 1 - Drums and Percussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated CUDA memory:  0.000 GiB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(f\"Allocated CUDA memory: {torch.cuda.memory_allocated() / 1024 ** 3:6.3f} GiB\")\n",
    "\n",
    "num_epochs = 10\n",
    "data_dir = \"/mnt/data/Daftset/Dataset\"\n",
    "tmp_dir = \"/mnt/data/Daftset/Dataset_tmp\"\n",
    "batch_size = 1\n",
    "learning_rate = 1e-3\n",
    "num_channels = 2\n",
    "freq_orig = 44100\n",
    "freq_scale = 2\n",
    "chunk_duration = 3\n",
    "weight_decay = 1e-4\n",
    "spectral_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, tmp_dir: str, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        self.data_dir = data_dir      # Directory containing input *and* label files\n",
    "        self.tmp_dir = tmp_dir        # Directory for temporary files\n",
    "        self.input_tail = input_tail  # File extensions for input files\n",
    "        self.label_tail = label_tail  # File extensions for label files\n",
    "        self.chunk_size = chunk_duration * (freq_orig // freq_scale)  # Number of samples per chunk\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)  # List of tuples: (input_filename, label_filename)\n",
    "        self.input_label_lengths = self._load_audio_lengths(self.input_label_pairs)  # List of tuples: (input_length, label_length)\n",
    "        self.input_length = sum([in_len for in_len, _ in self.input_label_lengths])  # Total number of samples\n",
    "        self.batch_count = 0  # 'Global' counter for batch generation\n",
    "\n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> list:\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        file_tuples = []\n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _load_audio_lengths(self, file_tuples: list) -> list:\n",
    "        lengths = []\n",
    "        for in_fname, lb_fname in file_tuples:\n",
    "            audio_in = torchaudio.load(os.path.join(self.data_dir, in_fname))[0]\n",
    "            audio_lb = torchaudio.load(os.path.join(self.data_dir, lb_fname))[0]\n",
    "            # Trim both audios to equal length\n",
    "            max_length = max(audio_in.shape[1], audio_lb.shape[1])\n",
    "            if audio_in.shape[1] < max_length:\n",
    "                audio_in = torch.nn.functional.pad(audio_in, (0, max_length - audio_in.shape[1]))\n",
    "            elif audio_lb.shape[1] < max_length:\n",
    "                audio_lb = torch.nn.functional.pad(audio_lb, (0, max_length - audio_lb.shape[1]))\n",
    "            tmp_path_in = os.path.join(self.tmp_dir, in_fname)\n",
    "            tmp_path_lb = os.path.join(self.tmp_dir, lb_fname)\n",
    "            torchaudio.save(tmp_path_in, audio_in, freq_orig // freq_scale)\n",
    "            torchaudio.save(tmp_path_lb, audio_lb, freq_orig // freq_scale)\n",
    "            lengths.append((max_length, max_length))\n",
    "        return lengths\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.input_length // self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunk, label_chunk = [], []\n",
    "        remaining_samples = self.chunk_size\n",
    "        while remaining_samples > 0:\n",
    "            for (in_file, lb_file), (audio_length, _) in zip(self.input_label_pairs, self.input_label_lengths):\n",
    "                if global_start >= audio_length:\n",
    "                    global_start -= audio_length\n",
    "                    continue\n",
    "                samples_from_file = min(audio_length - global_start, remaining_samples)\n",
    "                path_in = os.path.join(self.tmp_dir, in_file)\n",
    "                path_lb = os.path.join(self.tmp_dir, lb_file)\n",
    "                audio_in = torchaudio.load(path_in)[0][:, global_start:global_start + samples_from_file]\n",
    "                audio_lb = torchaudio.load(path_lb)[0][:, global_start:global_start + samples_from_file]\n",
    "                input_chunk.append(audio_in)\n",
    "                label_chunk.append(audio_lb)\n",
    "                remaining_samples -= samples_from_file\n",
    "                global_start = 0  # Reset for next file\n",
    "                if remaining_samples == 0:\n",
    "                    break\n",
    "            if remaining_samples > 0:\n",
    "                # Start over if we've gone through all files and still need more samples\n",
    "                global_start = 0\n",
    "        return torch.cat(input_chunk, dim=1), torch.cat(label_chunk, dim=1)\n",
    "\n",
    "    def get_batch(self, batch_size, randomized=False):\n",
    "        if randomized:\n",
    "            max_start_idx = len(self) - 1\n",
    "            idx = np.random.randint(0, max_start_idx + 1)\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "        batch = []\n",
    "        for i in range(batch_size):\n",
    "            current_idx = (idx + i) % len(self) # Wrap around\n",
    "            batch.append(self.__getitem__(current_idx))\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk Count: 2994 \n",
      "Input Tensor: torch.Size([2, 66150]) \n",
      "Label Tensor: torch.Size([2, 66150])\n",
      "Batch Input Tensor: torch.Size([1, 2, 66150]) \n",
      "Batch Label Tensor: torch.Size([1, 2, 66150])\n",
      "Loader Input Tensor: torch.Size([1, 2, 66150]) \n",
      "Loader Label Tensor: torch.Size([1, 2, 66150])\n",
      "\n",
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Direct Call Sanity Check\n",
    "input_audio, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "\n",
    "# Expect shape: [2, 66150]\n",
    "assert input_audio.shape == (2, 66150), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, 66150), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch(1)\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 66150]\n",
    "assert input_batch.shape == (1, 2, 66150), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (1, 2, 66150), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 66150]\n",
    "assert input_collate.shape == (1, 2, 66150), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (1, 2, 66150), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "assert torch.equal(input_audio, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input_audio, label_audio = dataset[i]\n",
    "    assert input_audio.shape == (2, 66150), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, 66150), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1)\n",
    "    assert input_batch.shape == (1, 2, 66150), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, 66150), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, 66150), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, 66150), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1, randomized=True)\n",
    "    assert input_batch.shape == (1, 2, 66150), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, 66150), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=16, num_layers=4):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(\n",
    "                nn.Conv1d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1)\n",
    "            )\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.final_conv = nn.Conv1d(in_channels + num_layers * growth_rate, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = torch.cat([out, layer(out)], dim=1)\n",
    "        return self.final_conv(out) + x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    # Simplified representation; adjust based on specific requirements or TasNet variant\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=8):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels) for _ in range(num_blocks)])\n",
    "        \n",
    "    def _build_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # Convert from (batch, channels, time) to (time, batch, channels)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.permute(1, 2, 0)  # Convert back to (batch, channels, time)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.down2 = DownSample(16, 32)\n",
    "        self.down3 = DownSample(32, 64)\n",
    "        self.cv_tasnet = CV_TasNet_Block(64, 64)\n",
    "        self.temporal_attention = TemporalSelfAttention(64)\n",
    "\n",
    "        # Multi-scale feature fusion\n",
    "        # Fuse upsampled features and skip connection from downsampled features\n",
    "        self.up3 = UpSample(64 + 64, 32)\n",
    "        self.up2 = UpSample(32 + 32, 16)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Conv1d(16 + 16, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(16, num_channels, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "                utils.weight_norm(m)\n",
    "\n",
    "    def forward(self, x): # x: (batch, channels, time)\n",
    "        skip1 = self.down1(x)\n",
    "        skip2 = self.down2(skip1)\n",
    "        skip3 = self.down3(skip2)\n",
    "        x = self.cv_tasnet(skip3)\n",
    "        x = self.temporal_attention(x)\n",
    "        x = self.up3(torch.cat([x, skip3], dim=1))\n",
    "        x = self.up2(torch.cat([x, skip2], dim=1))\n",
    "        x = self.up1(torch.cat([x, skip1], dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, n_fft=1024, hop_length=None):\n",
    "    output_stft = torch.stft(output, n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
    "    target_stft = torch.stft(target, n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
    "    spectral_diff = torch.abs(torch.abs(output_stft) - torch.abs(target_stft))\n",
    "    return torch.mean(spectral_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcus/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "model = AudioUNet(num_channels=2).to(device) # Adjust audio_length and num_speakers\n",
    "criterion_mse = nn.MSELoss() # Mean Squared Error for audio regression tasks\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: torch.Size([1, 2, 66150])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 18, 3], expected input[1, 36, 66150] to have 18 channels, but got 36 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m criterion_mse(outputs, labels)\n\u001b[1;32m     11\u001b[0m spec_loss \u001b[38;5;241m=\u001b[39m spectral_loss(outputs, labels, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 103\u001b[0m, in \u001b[0;36mAudioUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 103\u001b[0m     skip1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB:\u001b[39m\u001b[38;5;124m'\u001b[39m, skip1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    105\u001b[0m     skip2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(skip1)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mResidualDenseBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([out, \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(out) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 18, 3], expected input[1, 36, 66150] to have 18 channels, but got 36 channels instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        mse_loss = criterion_mse(outputs, labels)\n",
    "        spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "        loss = mse_loss + spectral_weight * spec_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if str(device) == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "    #if epoch % 2 == 0: # Validate every 2 epochs\n",
    "    #    model.eval()\n",
    "    #    with torch.no_grad():\n",
    "    #        val_loss = 0.0\n",
    "    #        for i, data in enumerate(val_loader, 0):\n",
    "    #            inputs, labels = data\n",
    "    #            inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #            outputs = model(inputs)\n",
    "    #            loss = criterion(outputs, labels)\n",
    "    #            val_loss += loss.item()\n",
    "    #        print(f'Validation loss: {val_loss / len(val_loader):.3f}')\n",
    "        \n",
    "    if epoch % 5 == 0: # Save every 5 epochs\n",
    "        torch.save(model.state_dict(), f'audio_unet_epoch_{epoch}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
