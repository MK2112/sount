{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing - Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch_optimizer as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple\n",
    "from scipy.io.wavfile import write\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:\t\t\tcuda\n",
      "Allocated CUDA memory:\t  0.0000 GiB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:\\t\\t\\t{device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(f\"Allocated CUDA memory:\\t{torch.cuda.memory_allocated() / 1024 ** 3:8.4f} GiB\")\n",
    "\n",
    "num_epochs = 100        # Number of epochs to train\n",
    "data_dir = \"/mnt/data/Daftset/Dataset\"  # Directory containing the dataset (we will work on a copy)\n",
    "batch_size = 1          # 3060 tackles single-entry batches at max\n",
    "learning_rate = 1e-3    # Light learning rate\n",
    "num_channels = 2        # Number of audio channels\n",
    "freq_orig = 44100       # Original frequency of the audio files\n",
    "optim_k = 5             # Average weight updates over optim_k steps to stabilize training\n",
    "optim_alpha = 0.3       # Weight of influence of Lookahead's fast weights on the slow weights\n",
    "chunk_duration = 2      # Duration of training examples in seconds -> samples_per_example = (freq_orig) * chunk_duration\n",
    "weight_decay = 1e-4     # Weight decay for the optimizer\n",
    "spectral_weight = 0.6   # Spectral loss impact for total loss calculation\n",
    "accum_steps = 256       # Effective_batch_size = batch_size * accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller datasets may afford us to load the entire dataset at once during `init`, but this set is custom and sizewise unpredictable enough to require on-the-fly loading.\n",
    "Thing is, if we strictly load on request, how can we provide a `__len__` method for the dataset?<br>\n",
    "Thankfully, audio files have metadata we can use to determine their length without loading them, based on which we can provide a `__len__` method.\n",
    "\n",
    "- `_process_files` loads the in-out file pairs by name and as a list of name pairs, but not the actual audio data. We need that for ordered access to the dataset.\n",
    "- `_get_file_info` takes this list of name pairs and loads just the metadata of the audio files, which we can use to determine the length of all files. Note that we expect input and output to be the same size here. This is unrealistic, but we can trim or pad the labels accordingly later when we actually load the data.\n",
    "- The main data providing complexity is in `__get_item__`, where we \n",
    "    - calculate the idx of the requested input, label pair at a global scale (across all audio)\n",
    "    - assemble the input and the label pair, even across different files (load until we reach the requested sample size)\n",
    "    - trim or pad the label to match the input size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, batch_size: int = batch_size, chunk_duration: float = chunk_duration, \n",
    "                 freq_orig: int = freq_orig, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_tail = input_tail\n",
    "        self.label_tail = label_tail\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.freq_orig = freq_orig\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)  # Get input-label file name pairs \n",
    "        self.file_info = self._get_file_info() # Sift only through metadata for each file\n",
    "        self.input_length = sum(info['length'] for info in self.file_info) # Calculate total length and chunk information (from metadata)\n",
    "        self.chunk_size = int(chunk_duration * freq_orig)  # Ensure chunk_size is an integer\n",
    "        self.batch_count = 0\n",
    "        self._file_handle_cache = {} # Create cache for file handles\n",
    "\n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> List[Tuple[str, str]]:\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        file_tuples = []\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        \n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _get_file_info(self) -> List[dict]:\n",
    "        file_info = []\n",
    "        for in_fname, lb_fname in self.input_label_pairs:\n",
    "            info = torchaudio.info(os.path.join(self.data_dir, in_fname))\n",
    "            length = info.num_frames\n",
    "            file_info.append({'length': length,\n",
    "                              'input_path': os.path.join(self.data_dir, in_fname),\n",
    "                              'label_path': os.path.join(self.data_dir, lb_fname)})\n",
    "        return file_info\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def _get_file_handle(self, file_path: str) -> sf.SoundFile:\n",
    "        if file_path not in self._file_handle_cache:\n",
    "            self._file_handle_cache[file_path] = sf.SoundFile(file_path, 'r')\n",
    "        return self._file_handle_cache[file_path]\n",
    "\n",
    "    def _normalize_audio(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        data = data.float()\n",
    "        if torch.abs(data).max() > 0:\n",
    "            data = data / (torch.abs(data).max() + 1e-8)\n",
    "        return torch.clamp(data, min=-0.99, max=0.99)  # Prevent extreme values\n",
    "\n",
    "    def _read_audio_chunk(self, file_path: str, start: int, length: int) -> torch.Tensor:\n",
    "        handle = self._get_file_handle(file_path)\n",
    "        handle.seek(start)\n",
    "        data = handle.read(length)\n",
    "        data = torch.from_numpy(data).T\n",
    "        return self._normalize_audio(data)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        total_chunks = self.input_length // self.chunk_size\n",
    "        return total_chunks // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunks, label_chunks = [], []\n",
    "        samples_remaining = self.chunk_size\n",
    "        cumulative_length = 0\n",
    "        current_file_index = 0  # Track the current file being processed\n",
    "\n",
    "        while samples_remaining > 0:\n",
    "            if current_file_index >= len(self.file_info):\n",
    "                current_file_index = 0  # Wrap around to the first file\n",
    "\n",
    "            file_info = self.file_info[current_file_index]\n",
    "            \n",
    "            # Check if we are past the current file\n",
    "            if global_start >= cumulative_length + file_info['length']:\n",
    "                cumulative_length += file_info['length']\n",
    "                current_file_index += 1  # Move to the next file\n",
    "                continue\n",
    "            \n",
    "            local_start = global_start - cumulative_length\n",
    "            if local_start < 0:\n",
    "                local_start = 0\n",
    "\n",
    "            samples_from_file = min(file_info['length'] - local_start, samples_remaining)\n",
    "\n",
    "            if samples_from_file > 0:\n",
    "                input_chunk = self._read_audio_chunk(file_info['input_path'], local_start, samples_from_file)\n",
    "                label_chunk = self._read_audio_chunk(file_info['label_path'], local_start, samples_from_file)\n",
    "                input_chunks.append(input_chunk)\n",
    "                label_chunks.append(label_chunk)\n",
    "                samples_remaining -= samples_from_file\n",
    "            \n",
    "            cumulative_length += file_info['length']\n",
    "            current_file_index += 1  # Move to the next file after processing\n",
    "            \n",
    "        if samples_remaining > 0:\n",
    "            raise ValueError(\"Not enough samples available to fulfill the request.\")  # Handle as needed\n",
    "\n",
    "        return (torch.cat(input_chunks, dim=1), torch.cat(label_chunks, dim=1))\n",
    "\n",
    "    def get_batch(self, batch_size: int, randomized: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if randomized:\n",
    "            idx = np.random.randint(0, len(self))\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "                \n",
    "        batch = [self.__getitem__((idx + i) % len(self)) for i in range(batch_size)]\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        \n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "            \n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    def __del__(self):\n",
    "        for handle in self._file_handle_cache.values():\n",
    "            handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk Count: 2345 \n",
      "Input Tensor: torch.Size([2, 88200]) \n",
      "Label Tensor: torch.Size([2, 88200])\n",
      "Batch Input Tensor: torch.Size([1, 2, 88200]) \n",
      "Batch Label Tensor: torch.Size([1, 2, 88200])\n",
      "Loader Input Tensor: torch.Size([1, 2, 88200]) \n",
      "Loader Label Tensor: torch.Size([1, 2, 88200])\n",
      "\n",
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Direct Call Sanity Check\n",
    "input_audio, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "# Expect shape: [2, 44100]\n",
    "assert input_audio.shape == (2, assert_f), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, assert_f), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch(1)\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_batch.shape == (1, 2, assert_f), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (1, 2, assert_f), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_collate.shape == (1, 2, assert_f), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (1, 2, assert_f), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "assert torch.equal(input_audio, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input_audio, label_audio = dataset[i]\n",
    "    assert input_audio.shape == (2, assert_f), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, assert_f), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, assert_f), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1, randomized=True)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "# Taking a listen ensures consistent pair assembly\n",
    "input_audio, label_audio = dataset[np.random.randint(0, len(dataset)-1)]\n",
    "input_audio = input_audio.T.numpy()\n",
    "label_audio = label_audio.T.numpy()\n",
    "write('input_sample.wav', freq_orig, input_audio)\n",
    "write('label_sample.wav', freq_orig, label_audio)\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=32, num_layers=4):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_channels + i * growth_rate\n",
    "            self.norms.append(nn.InstanceNorm1d(in_ch))\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv1d(in_ch, growth_rate, kernel_size=3, padding=1),\n",
    "                nn.InstanceNorm1d(growth_rate),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.1)))\n",
    "        final_ch = in_channels + num_layers * growth_rate\n",
    "        self.final_norm = nn.InstanceNorm1d(final_ch)\n",
    "        self.final_conv = nn.Conv1d(final_ch, in_channels, kernel_size=1)\n",
    "        self.output_norm = nn.InstanceNorm1d(in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for norm, layer in zip(self.norms, self.layers):\n",
    "            inputs = torch.cat(features, dim=1)\n",
    "            inputs = norm(inputs)\n",
    "            out = layer(inputs)\n",
    "            features.append(out)\n",
    "        out = self.final_norm(torch.cat(features, dim=1))\n",
    "        out = self.final_conv(out)\n",
    "        out = self.output_norm(out)\n",
    "        return (out * 0.5) + x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Resizer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, target_size):\n",
    "        super(Resizer, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channel, out_channel, kernel_size=4, stride=2, padding=1)\n",
    "        self.target_size = target_size\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=self.target_size, mode='linear', align_corners=True)\n",
    "        return self.activation(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        diff = x.size(2) - skip.size(2)\n",
    "        if diff > 0:\n",
    "            x = x[:, :, :skip.size(2)]\n",
    "        elif diff < 0:\n",
    "            x = nn.functional.pad(x, (0, -diff))\n",
    "        return x\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=12):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels) for _ in range(num_blocks)])\n",
    "        \n",
    "    def _build_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x.permute(1, 2, 0)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.down2 = DownSample(2, 4)\n",
    "        self.down3 = DownSample(4, 8)\n",
    "        self.down4 = DownSample(8, 16)\n",
    "        self.cv_tasnet = CV_TasNet_Block(16, 16)\n",
    "        self.temporal_attention = TemporalSelfAttention(16)\n",
    "        self.up4 = UpSample(16, 8)\n",
    "        self.up3 = UpSample(8, 4)\n",
    "        self.up2 = UpSample(4, 2)\n",
    "        self.resizer = Resizer(2, 2, assert_f)\n",
    "        self.up1 = nn.Sequential(nn.Conv1d(4, 4, kernel_size=3, padding=1),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(4, num_channels, kernel_size=3, padding=1),\n",
    "                                 nn.Tanh()) # Normalize to [-1, 1] for audio\n",
    " \n",
    "    def forward(self, x):\n",
    "        x_orig = x\n",
    "        skip1 = self.down1(x)            # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        skip2 = self.down2(skip1)        # (batch, 4, ((freq_orig) * chunk_duration) / 2)\n",
    "        skip3 = self.down3(skip2)        # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        skip4 = self.down4(skip3)        # (batch, 16, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.cv_tasnet(skip4)        # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.temporal_attention(x)   # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.up4(x, skip4)           # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up3(x, skip3)           # (batch, 4, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up2(x, skip2)           # (batch, 2, ((freq_orig) * chunk_duration) / 2)\n",
    "        x = self.resizer(x)              # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        x = torch.cat([x, skip1], dim=1) # (batch, 4, (freq_orig) * chunk_duration)\n",
    "        x = self.up1(x)                  # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        return x + x_orig                # x applies the morphs to the original input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_splits(dataset, val_ratio=0.2, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_ratio * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    return SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "train_sampler, val_sampler = create_train_val_splits(dataset, val_ratio=0.2, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, n_fft=1024, hop_length=None, epsilon=1e-8):\n",
    "    if hop_length is None:\n",
    "        hop_length = n_fft // 4\n",
    "    window = torch.hann_window(n_fft).to(output.device)\n",
    "    def safe_log(x):\n",
    "        return torch.log(torch.clamp(x, min=epsilon))\n",
    "    loss = 0\n",
    "    for i in range(output.shape[1]):\n",
    "        output_stft = torch.stft(output[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        target_stft = torch.stft(target[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        output_mag, target_mag = torch.abs(output_stft), torch.abs(target_stft)\n",
    "        output_log_mag, target_log_mag = safe_log(output_mag + epsilon), safe_log(target_mag + epsilon)\n",
    "        mag_loss = torch.mean(torch.abs(output_log_mag - target_log_mag))\n",
    "        output_phase, target_phase = torch.angle(output_stft), torch.angle(target_stft)\n",
    "        phase_loss = 1 - torch.mean(torch.cos(output_phase - target_phase))\n",
    "        loss += mag_loss + 0.1 * phase_loss\n",
    "    return loss / output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 46,096\n"
     ]
    }
   ],
   "source": [
    "model = AudioUNet(num_channels=2).to(device).float() # Adjust audio_length and num_speakers\n",
    "model = torch.compile(model)\n",
    "criterion_mse = nn.MSELoss().to(device) # Mean Squared Error\n",
    "base_optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=1e-8)\n",
    "\n",
    "# Switching between providing 'fast weights' and 'slow weights' for AdamW optimizer update calculations\n",
    "optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n",
    "\n",
    "# Gradually warm and then cool down LR over time\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader) // accum_steps,\n",
    "                       pct_start=0.1, anneal_strategy='cos', div_factor=10.0, final_div_factor=1000.0)\n",
    "\n",
    "print(f\"Model Parameter Count: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Batch Size: 256 examples\n",
      "Epoch [  1/100] | Mini-Batch [ 512/2345] | Train: 2.584758 | LR: 0.000204\n",
      "Epoch [  1/100] | Mini-Batch [1024/2345] | Train: 2.547614 | LR: 0.000215\n",
      "Epoch [  1/100] | Mini-Batch [1536/2345] | Train: 2.465619 | LR: 0.000233\n",
      "Epoch [  1/100] | Validation: 1.910097\n",
      "Epoch [  2/100] | Mini-Batch [ 512/2345] | Train: 2.485328 | LR: 0.000275\n",
      "Epoch [  2/100] | Mini-Batch [1024/2345] | Train: 2.376441 | LR: 0.000311\n",
      "Epoch [  2/100] | Mini-Batch [1536/2345] | Train: 2.467648 | LR: 0.000353\n",
      "Epoch [  2/100] | Validation: 1.771295\n",
      "Epoch [  3/100] | Mini-Batch [ 512/2345] | Train: 2.314434 | LR: 0.000428\n",
      "Epoch [  3/100] | Mini-Batch [1024/2345] | Train: 2.346556 | LR: 0.000486\n",
      "Epoch [  3/100] | Mini-Batch [1536/2345] | Train: 2.236746 | LR: 0.000548\n",
      "Epoch [  3/100] | Validation: 1.752925\n",
      "Epoch [  4/100] | Mini-Batch [ 512/2345] | Train: 2.254073 | LR: 0.000650\n",
      "Epoch [  4/100] | Mini-Batch [1024/2345] | Train: 2.127245 | LR: 0.000723\n",
      "Epoch [  4/100] | Mini-Batch [1536/2345] | Train: 2.169895 | LR: 0.000799\n",
      "Epoch [  4/100] | Validation: 1.649621\n",
      "Epoch [  5/100] | Mini-Batch [ 512/2345] | Train: 2.063485 | LR: 0.000917\n",
      "Epoch [  5/100] | Mini-Batch [1024/2345] | Train: 2.070338 | LR: 0.000998\n",
      "Epoch [  5/100] | Mini-Batch [1536/2345] | Train: 2.012651 | LR: 0.001080\n",
      "Epoch [  5/100] | Validation: 1.596624\n",
      "Epoch [  6/100] | Mini-Batch [ 512/2345] | Train: 1.955225 | LR: 0.001202\n",
      "Epoch [  6/100] | Mini-Batch [1024/2345] | Train: 1.936250 | LR: 0.001283\n",
      "Epoch [  6/100] | Mini-Batch [1536/2345] | Train: 1.868482 | LR: 0.001362\n",
      "Epoch [  6/100] | Validation: 1.585174\n",
      "Epoch [  7/100] | Mini-Batch [ 512/2345] | Train: 1.868598 | LR: 0.001477\n",
      "Epoch [  7/100] | Mini-Batch [1024/2345] | Train: 1.815494 | LR: 0.001550\n",
      "Epoch [  7/100] | Mini-Batch [1536/2345] | Train: 1.819149 | LR: 0.001619\n",
      "Epoch [  7/100] | Validation: 1.547610\n",
      "Epoch [  8/100] | Mini-Batch [ 512/2345] | Train: 1.743362 | LR: 0.001714\n",
      "Epoch [  8/100] | Mini-Batch [1024/2345] | Train: 1.806034 | LR: 0.001772\n",
      "Epoch [  8/100] | Mini-Batch [1536/2345] | Train: 1.733938 | LR: 0.001823\n",
      "Epoch [  8/100] | Validation: 1.541083\n",
      "Epoch [  9/100] | Mini-Batch [ 512/2345] | Train: 1.736313 | LR: 0.001889\n",
      "Epoch [  9/100] | Mini-Batch [1024/2345] | Train: 1.704305 | LR: 0.001925\n",
      "Epoch [  9/100] | Mini-Batch [1536/2345] | Train: 1.678358 | LR: 0.001955\n",
      "Epoch [  9/100] | Validation: 1.460552\n",
      "Epoch [ 10/100] | Mini-Batch [ 512/2345] | Train: 1.643433 | LR: 0.001985\n",
      "Epoch [ 10/100] | Mini-Batch [1024/2345] | Train: 1.645359 | LR: 0.001996\n",
      "Epoch [ 10/100] | Mini-Batch [1536/2345] | Train: 1.618806 | LR: 0.002000\n",
      "Epoch [ 10/100] | Validation: 1.340214\n",
      "Epoch [ 11/100] | Mini-Batch [ 512/2345] | Train: 1.614009 | LR: 0.002000\n",
      "Epoch [ 11/100] | Mini-Batch [1024/2345] | Train: 1.594675 | LR: 0.002000\n",
      "Epoch [ 11/100] | Mini-Batch [1536/2345] | Train: 1.545130 | LR: 0.001999\n",
      "Epoch [ 11/100] | Validation: 1.346945\n",
      "Epoch [ 12/100] | Mini-Batch [ 512/2345] | Train: 1.569690 | LR: 0.001999\n",
      "Epoch [ 12/100] | Mini-Batch [1024/2345] | Train: 1.497696 | LR: 0.001998\n",
      "Epoch [ 12/100] | Mini-Batch [1536/2345] | Train: 1.548714 | LR: 0.001998\n",
      "Epoch [ 12/100] | Validation: 1.297548\n",
      "Epoch [ 13/100] | Mini-Batch [ 512/2345] | Train: 1.483627 | LR: 0.001996\n",
      "Epoch [ 13/100] | Mini-Batch [1024/2345] | Train: 1.517811 | LR: 0.001996\n",
      "Epoch [ 13/100] | Mini-Batch [1536/2345] | Train: 1.471322 | LR: 0.001995\n",
      "Epoch [ 13/100] | Validation: 1.292412\n",
      "Epoch [ 14/100] | Mini-Batch [ 512/2345] | Train: 1.469734 | LR: 0.001993\n",
      "Epoch [ 14/100] | Mini-Batch [1024/2345] | Train: 1.461117 | LR: 0.001992\n",
      "Epoch [ 14/100] | Mini-Batch [1536/2345] | Train: 1.452317 | LR: 0.001990\n",
      "Epoch [ 14/100] | Validation: 1.272298\n",
      "Epoch [ 15/100] | Mini-Batch [ 512/2345] | Train: 1.440513 | LR: 0.001988\n",
      "Epoch [ 15/100] | Mini-Batch [1024/2345] | Train: 1.423398 | LR: 0.001986\n",
      "Epoch [ 15/100] | Mini-Batch [1536/2345] | Train: 1.422497 | LR: 0.001985\n",
      "Epoch [ 15/100] | Validation: 1.259069\n",
      "Epoch [ 16/100] | Mini-Batch [ 512/2345] | Train: 1.402197 | LR: 0.001982\n",
      "Epoch [ 16/100] | Mini-Batch [1024/2345] | Train: 1.396862 | LR: 0.001980\n",
      "Epoch [ 16/100] | Mini-Batch [1536/2345] | Train: 1.384695 | LR: 0.001978\n",
      "Epoch [ 16/100] | Validation: 1.258386\n",
      "Epoch [ 17/100] | Mini-Batch [ 512/2345] | Train: 1.389633 | LR: 0.001975\n",
      "Epoch [ 17/100] | Mini-Batch [1024/2345] | Train: 1.347616 | LR: 0.001973\n",
      "Epoch [ 17/100] | Mini-Batch [1536/2345] | Train: 1.378369 | LR: 0.001970\n",
      "Epoch [ 17/100] | Validation: 1.240045\n",
      "Epoch [ 18/100] | Mini-Batch [ 512/2345] | Train: 1.357341 | LR: 0.001967\n",
      "Epoch [ 18/100] | Mini-Batch [1024/2345] | Train: 1.370385 | LR: 0.001964\n",
      "Epoch [ 18/100] | Mini-Batch [1536/2345] | Train: 1.332457 | LR: 0.001961\n",
      "Epoch [ 18/100] | Validation: 1.238733\n",
      "Epoch [ 19/100] | Mini-Batch [ 512/2345] | Train: 1.348342 | LR: 0.001957\n",
      "Epoch [ 19/100] | Mini-Batch [1024/2345] | Train: 1.303935 | LR: 0.001954\n",
      "Epoch [ 19/100] | Mini-Batch [1536/2345] | Train: 1.319354 | LR: 0.001951\n",
      "Epoch [ 19/100] | Validation: 1.221675\n",
      "Epoch [ 20/100] | Mini-Batch [ 512/2345] | Train: 1.322759 | LR: 0.001946\n",
      "Epoch [ 20/100] | Mini-Batch [1024/2345] | Train: 1.302997 | LR: 0.001943\n",
      "Epoch [ 20/100] | Mini-Batch [1536/2345] | Train: 1.298952 | LR: 0.001940\n",
      "Epoch [ 20/100] | Validation: 1.206271\n",
      "Epoch [ 21/100] | Mini-Batch [ 512/2345] | Train: 1.283509 | LR: 0.001934\n",
      "Epoch [ 21/100] | Mini-Batch [1024/2345] | Train: 1.289825 | LR: 0.001931\n",
      "Epoch [ 21/100] | Mini-Batch [1536/2345] | Train: 1.290730 | LR: 0.001927\n",
      "Epoch [ 21/100] | Validation: 1.209628\n",
      "Epoch [ 22/100] | Mini-Batch [ 512/2345] | Train: 1.289757 | LR: 0.001921\n",
      "Epoch [ 22/100] | Mini-Batch [1024/2345] | Train: 1.280106 | LR: 0.001918\n",
      "Epoch [ 22/100] | Mini-Batch [1536/2345] | Train: 1.270612 | LR: 0.001914\n",
      "Epoch [ 22/100] | Validation: 1.192565\n",
      "Epoch [ 23/100] | Mini-Batch [ 512/2345] | Train: 1.257156 | LR: 0.001907\n",
      "Epoch [ 23/100] | Mini-Batch [1024/2345] | Train: 1.271483 | LR: 0.001903\n",
      "Epoch [ 23/100] | Mini-Batch [1536/2345] | Train: 1.253905 | LR: 0.001899\n",
      "Epoch [ 23/100] | Validation: 1.193766\n",
      "Epoch [ 24/100] | Mini-Batch [ 512/2345] | Train: 1.256319 | LR: 0.001892\n",
      "Epoch [ 24/100] | Mini-Batch [1024/2345] | Train: 1.246918 | LR: 0.001888\n",
      "Epoch [ 24/100] | Mini-Batch [1536/2345] | Train: 1.242324 | LR: 0.001883\n",
      "Epoch [ 24/100] | Validation: 1.179265\n",
      "Epoch [ 25/100] | Mini-Batch [ 512/2345] | Train: 1.240665 | LR: 0.001876\n",
      "Epoch [ 25/100] | Mini-Batch [1024/2345] | Train: 1.230600 | LR: 0.001871\n",
      "Epoch [ 25/100] | Mini-Batch [1536/2345] | Train: 1.230237 | LR: 0.001866\n",
      "Epoch [ 25/100] | Validation: 1.167001\n",
      "Epoch [ 26/100] | Mini-Batch [ 512/2345] | Train: 1.207991 | LR: 0.001858\n",
      "Epoch [ 26/100] | Mini-Batch [1024/2345] | Train: 1.232070 | LR: 0.001853\n",
      "Epoch [ 26/100] | Mini-Batch [1536/2345] | Train: 1.215119 | LR: 0.001848\n",
      "Epoch [ 26/100] | Validation: 1.167861\n",
      "Epoch [ 27/100] | Mini-Batch [ 512/2345] | Train: 1.209837 | LR: 0.001840\n",
      "Epoch [ 27/100] | Mini-Batch [1024/2345] | Train: 1.207088 | LR: 0.001835\n",
      "Epoch [ 27/100] | Mini-Batch [1536/2345] | Train: 1.204560 | LR: 0.001829\n",
      "Epoch [ 27/100] | Validation: 1.154685\n",
      "Epoch [ 28/100] | Mini-Batch [ 512/2345] | Train: 1.189165 | LR: 0.001821\n",
      "Epoch [ 28/100] | Mini-Batch [1024/2345] | Train: 1.202605 | LR: 0.001815\n",
      "Epoch [ 28/100] | Mini-Batch [1536/2345] | Train: 1.202263 | LR: 0.001809\n",
      "Epoch [ 28/100] | Validation: 1.154624\n",
      "Epoch [ 29/100] | Mini-Batch [ 512/2345] | Train: 1.192813 | LR: 0.001800\n",
      "Epoch [ 29/100] | Mini-Batch [1024/2345] | Train: 1.200595 | LR: 0.001794\n",
      "Epoch [ 29/100] | Mini-Batch [1536/2345] | Train: 1.180096 | LR: 0.001788\n",
      "Epoch [ 29/100] | Validation: 1.143568\n",
      "Epoch [ 30/100] | Mini-Batch [ 512/2345] | Train: 1.169256 | LR: 0.001779\n",
      "Epoch [ 30/100] | Mini-Batch [1024/2345] | Train: 1.179204 | LR: 0.001772\n",
      "Epoch [ 30/100] | Mini-Batch [1536/2345] | Train: 1.177571 | LR: 0.001766\n",
      "Epoch [ 30/100] | Validation: 1.132443\n",
      "Epoch [ 31/100] | Mini-Batch [ 512/2345] | Train: 1.175697 | LR: 0.001756\n",
      "Epoch [ 31/100] | Mini-Batch [1024/2345] | Train: 1.161383 | LR: 0.001750\n",
      "Epoch [ 31/100] | Mini-Batch [1536/2345] | Train: 1.163274 | LR: 0.001743\n",
      "Epoch [ 31/100] | Validation: 1.132640\n",
      "Epoch [ 32/100] | Mini-Batch [ 512/2345] | Train: 1.154885 | LR: 0.001733\n",
      "Epoch [ 32/100] | Mini-Batch [1024/2345] | Train: 1.165500 | LR: 0.001726\n",
      "Epoch [ 32/100] | Mini-Batch [1536/2345] | Train: 1.165964 | LR: 0.001719\n",
      "Epoch [ 32/100] | Validation: 1.121934\n",
      "Epoch [ 33/100] | Mini-Batch [ 512/2345] | Train: 1.156451 | LR: 0.001709\n",
      "Epoch [ 33/100] | Mini-Batch [1024/2345] | Train: 1.161588 | LR: 0.001702\n",
      "Epoch [ 33/100] | Mini-Batch [1536/2345] | Train: 1.141498 | LR: 0.001695\n",
      "Epoch [ 33/100] | Validation: 1.122144\n",
      "Epoch [ 34/100] | Mini-Batch [ 512/2345] | Train: 1.137643 | LR: 0.001684\n",
      "Epoch [ 34/100] | Mini-Batch [1024/2345] | Train: 1.151618 | LR: 0.001677\n",
      "Epoch [ 34/100] | Mini-Batch [1536/2345] | Train: 1.147509 | LR: 0.001669\n",
      "Epoch [ 34/100] | Validation: 1.118238\n",
      "Epoch [ 35/100] | Mini-Batch [ 512/2345] | Train: 1.144183 | LR: 0.001658\n",
      "Epoch [ 35/100] | Mini-Batch [1024/2345] | Train: 1.141246 | LR: 0.001650\n",
      "Epoch [ 35/100] | Mini-Batch [1536/2345] | Train: 1.148694 | LR: 0.001643\n",
      "Epoch [ 35/100] | Validation: 1.110196\n",
      "Epoch [ 36/100] | Mini-Batch [ 512/2345] | Train: 1.144908 | LR: 0.001631\n",
      "Epoch [ 36/100] | Mini-Batch [1024/2345] | Train: 1.135075 | LR: 0.001624\n",
      "Epoch [ 36/100] | Mini-Batch [1536/2345] | Train: 1.119484 | LR: 0.001616\n",
      "Epoch [ 36/100] | Validation: 1.108470\n",
      "Epoch [ 37/100] | Mini-Batch [ 512/2345] | Train: 1.127115 | LR: 0.001604\n",
      "Epoch [ 37/100] | Mini-Batch [1024/2345] | Train: 1.132950 | LR: 0.001596\n",
      "Epoch [ 37/100] | Mini-Batch [1536/2345] | Train: 1.140735 | LR: 0.001588\n",
      "Epoch [ 37/100] | Validation: 1.101783\n",
      "Epoch [ 38/100] | Mini-Batch [ 512/2345] | Train: 1.121365 | LR: 0.001576\n",
      "Epoch [ 38/100] | Mini-Batch [1024/2345] | Train: 1.130934 | LR: 0.001567\n",
      "Epoch [ 38/100] | Mini-Batch [1536/2345] | Train: 1.128726 | LR: 0.001559\n",
      "Epoch [ 38/100] | Validation: 1.102245\n",
      "Epoch [ 39/100] | Mini-Batch [ 512/2345] | Train: 1.116704 | LR: 0.001547\n",
      "Epoch [ 39/100] | Mini-Batch [1024/2345] | Train: 1.121017 | LR: 0.001538\n",
      "Epoch [ 39/100] | Mini-Batch [1536/2345] | Train: 1.113319 | LR: 0.001530\n",
      "Epoch [ 39/100] | Validation: 1.095507\n",
      "Epoch [ 40/100] | Mini-Batch [ 512/2345] | Train: 1.124968 | LR: 0.001517\n",
      "Epoch [ 40/100] | Mini-Batch [1024/2345] | Train: 1.106382 | LR: 0.001509\n",
      "Epoch [ 40/100] | Mini-Batch [1536/2345] | Train: 1.103893 | LR: 0.001500\n",
      "Epoch [ 40/100] | Validation: 1.087540\n",
      "Epoch [ 41/100] | Mini-Batch [ 512/2345] | Train: 1.106014 | LR: 0.001487\n",
      "Epoch [ 41/100] | Mini-Batch [1024/2345] | Train: 1.109708 | LR: 0.001478\n",
      "Epoch [ 41/100] | Mini-Batch [1536/2345] | Train: 1.103756 | LR: 0.001470\n",
      "Epoch [ 41/100] | Validation: 1.087912\n",
      "Epoch [ 42/100] | Mini-Batch [ 512/2345] | Train: 1.111850 | LR: 0.001456\n",
      "Epoch [ 42/100] | Mini-Batch [1024/2345] | Train: 1.096510 | LR: 0.001447\n",
      "Epoch [ 42/100] | Mini-Batch [1536/2345] | Train: 1.106549 | LR: 0.001438\n",
      "Epoch [ 42/100] | Validation: 1.081455\n",
      "Epoch [ 43/100] | Mini-Batch [ 512/2345] | Train: 1.100977 | LR: 0.001425\n",
      "Epoch [ 43/100] | Mini-Batch [1024/2345] | Train: 1.096637 | LR: 0.001416\n",
      "Epoch [ 43/100] | Mini-Batch [1536/2345] | Train: 1.093889 | LR: 0.001407\n",
      "Epoch [ 43/100] | Validation: 1.082049\n",
      "Epoch [ 44/100] | Mini-Batch [ 512/2345] | Train: 1.099937 | LR: 0.001393\n",
      "Epoch [ 44/100] | Mini-Batch [1024/2345] | Train: 1.106888 | LR: 0.001384\n",
      "Epoch [ 44/100] | Mini-Batch [1536/2345] | Train: 1.100619 | LR: 0.001375\n",
      "Epoch [ 44/100] | Validation: 1.076205\n",
      "Epoch [ 45/100] | Mini-Batch [ 512/2345] | Train: 1.091179 | LR: 0.001361\n",
      "Epoch [ 45/100] | Mini-Batch [1024/2345] | Train: 1.092947 | LR: 0.001351\n",
      "Epoch [ 45/100] | Mini-Batch [1536/2345] | Train: 1.093825 | LR: 0.001342\n",
      "Epoch [ 45/100] | Validation: 1.072702\n",
      "Epoch [ 46/100] | Mini-Batch [ 512/2345] | Train: 1.092192 | LR: 0.001328\n",
      "Epoch [ 46/100] | Mini-Batch [1024/2345] | Train: 1.085683 | LR: 0.001319\n",
      "Epoch [ 46/100] | Mini-Batch [1536/2345] | Train: 1.092836 | LR: 0.001309\n",
      "Epoch [ 46/100] | Validation: 1.073068\n",
      "Epoch [ 47/100] | Mini-Batch [ 512/2345] | Train: 1.102073 | LR: 0.001295\n",
      "Epoch [ 47/100] | Mini-Batch [1024/2345] | Train: 1.079292 | LR: 0.001285\n",
      "Epoch [ 47/100] | Mini-Batch [1536/2345] | Train: 1.080411 | LR: 0.001276\n",
      "Epoch [ 47/100] | Validation: 1.065306\n",
      "Epoch [ 48/100] | Mini-Batch [ 512/2345] | Train: 1.082481 | LR: 0.001261\n",
      "Epoch [ 48/100] | Mini-Batch [1024/2345] | Train: 1.088863 | LR: 0.001252\n",
      "Epoch [ 48/100] | Mini-Batch [1536/2345] | Train: 1.084920 | LR: 0.001242\n",
      "Epoch [ 48/100] | Validation: 1.066186\n",
      "Epoch [ 49/100] | Mini-Batch [ 512/2345] | Train: 1.073786 | LR: 0.001227\n",
      "Epoch [ 49/100] | Mini-Batch [1024/2345] | Train: 1.075669 | LR: 0.001218\n",
      "Epoch [ 49/100] | Mini-Batch [1536/2345] | Train: 1.086381 | LR: 0.001208\n",
      "Epoch [ 49/100] | Validation: 1.059503\n",
      "Epoch [ 50/100] | Mini-Batch [ 512/2345] | Train: 1.084456 | LR: 0.001193\n",
      "Epoch [ 50/100] | Mini-Batch [1024/2345] | Train: 1.072274 | LR: 0.001184\n",
      "Epoch [ 50/100] | Mini-Batch [1536/2345] | Train: 1.077902 | LR: 0.001174\n",
      "Epoch [ 50/100] | Validation: 1.053229\n",
      "Epoch [ 51/100] | Mini-Batch [ 512/2345] | Train: 1.061132 | LR: 0.001159\n",
      "Epoch [ 51/100] | Mini-Batch [1024/2345] | Train: 1.083671 | LR: 0.001149\n",
      "Epoch [ 51/100] | Mini-Batch [1536/2345] | Train: 1.084417 | LR: 0.001139\n",
      "Epoch [ 51/100] | Validation: 1.054461\n",
      "Epoch [ 52/100] | Mini-Batch [ 512/2345] | Train: 1.089643 | LR: 0.001124\n",
      "Epoch [ 52/100] | Mini-Batch [1024/2345] | Train: 1.077246 | LR: 0.001115\n",
      "Epoch [ 52/100] | Mini-Batch [1536/2345] | Train: 1.069750 | LR: 0.001105\n",
      "Epoch [ 52/100] | Validation: 1.050322\n",
      "Epoch [ 53/100] | Mini-Batch [ 512/2345] | Train: 1.069375 | LR: 0.001090\n",
      "Epoch [ 53/100] | Mini-Batch [1024/2345] | Train: 1.071890 | LR: 0.001080\n",
      "Epoch [ 53/100] | Mini-Batch [1536/2345] | Train: 1.071975 | LR: 0.001070\n",
      "Epoch [ 53/100] | Validation: 1.053106\n",
      "Epoch [ 54/100] | Mini-Batch [ 512/2345] | Train: 1.063071 | LR: 0.001055\n",
      "Epoch [ 54/100] | Mini-Batch [1024/2345] | Train: 1.074371 | LR: 0.001045\n",
      "Epoch [ 54/100] | Mini-Batch [1536/2345] | Train: 1.077242 | LR: 0.001035\n",
      "Epoch [ 54/100] | Validation: 1.051056\n",
      "Epoch [ 55/100] | Mini-Batch [ 512/2345] | Train: 1.076870 | LR: 0.001020\n",
      "Epoch [ 55/100] | Mini-Batch [1024/2345] | Train: 1.069827 | LR: 0.001010\n",
      "Epoch [ 55/100] | Mini-Batch [1536/2345] | Train: 1.061088 | LR: 0.001000\n",
      "Epoch [ 55/100] | Validation: 1.047789\n",
      "Epoch [ 56/100] | Mini-Batch [ 512/2345] | Train: 1.071867 | LR: 0.000985\n",
      "Epoch [ 56/100] | Mini-Batch [1024/2345] | Train: 1.082278 | LR: 0.000975\n",
      "Epoch [ 56/100] | Mini-Batch [1536/2345] | Train: 1.063383 | LR: 0.000965\n",
      "Epoch [ 56/100] | Validation: 1.047910\n",
      "Epoch [ 57/100] | Mini-Batch [ 512/2345] | Train: 1.076664 | LR: 0.000950\n",
      "Epoch [ 57/100] | Mini-Batch [1024/2345] | Train: 1.069954 | LR: 0.000940\n",
      "Epoch [ 57/100] | Mini-Batch [1536/2345] | Train: 1.061025 | LR: 0.000930\n",
      "Epoch [ 57/100] | Validation: 1.044780\n",
      "Epoch [ 58/100] | Mini-Batch [ 512/2345] | Train: 1.063840 | LR: 0.000915\n",
      "Epoch [ 58/100] | Mini-Batch [1024/2345] | Train: 1.065569 | LR: 0.000906\n",
      "Epoch [ 58/100] | Mini-Batch [1536/2345] | Train: 1.055811 | LR: 0.000896\n",
      "Epoch [ 58/100] | Validation: 1.044894\n",
      "Epoch [ 59/100] | Mini-Batch [ 512/2345] | Train: 1.070945 | LR: 0.000881\n",
      "Epoch [ 59/100] | Mini-Batch [1024/2345] | Train: 1.061317 | LR: 0.000871\n",
      "Epoch [ 59/100] | Mini-Batch [1536/2345] | Train: 1.062697 | LR: 0.000861\n",
      "Epoch [ 59/100] | Validation: 1.042199\n",
      "Epoch [ 60/100] | Mini-Batch [ 512/2345] | Train: 1.061985 | LR: 0.000846\n",
      "Epoch [ 60/100] | Mini-Batch [1024/2345] | Train: 1.051985 | LR: 0.000836\n",
      "Epoch [ 60/100] | Mini-Batch [1536/2345] | Train: 1.078465 | LR: 0.000826\n",
      "Epoch [ 60/100] | Validation: 1.043089\n",
      "Epoch [ 61/100] | Mini-Batch [ 512/2345] | Train: 1.066890 | LR: 0.000812\n",
      "Epoch [ 61/100] | Mini-Batch [1024/2345] | Train: 1.061876 | LR: 0.000802\n",
      "Epoch [ 61/100] | Mini-Batch [1536/2345] | Train: 1.063299 | LR: 0.000792\n",
      "Epoch [ 61/100] | Validation: 1.042645\n",
      "Epoch [ 62/100] | Mini-Batch [ 512/2345] | Train: 1.072634 | LR: 0.000778\n",
      "Epoch [ 62/100] | Mini-Batch [1024/2345] | Train: 1.060420 | LR: 0.000768\n",
      "Epoch [ 62/100] | Mini-Batch [1536/2345] | Train: 1.059935 | LR: 0.000758\n",
      "Epoch [ 62/100] | Validation: 1.042285\n",
      "Epoch [ 63/100] | Mini-Batch [ 512/2345] | Train: 1.065784 | LR: 0.000744\n",
      "Epoch [ 63/100] | Mini-Batch [1024/2345] | Train: 1.052207 | LR: 0.000734\n",
      "Epoch [ 63/100] | Mini-Batch [1536/2345] | Train: 1.055236 | LR: 0.000724\n",
      "Epoch [ 63/100] | Validation: 1.042084\n",
      "Epoch [ 64/100] | Mini-Batch [ 512/2345] | Train: 1.068207 | LR: 0.000710\n",
      "Epoch [ 64/100] | Mini-Batch [1024/2345] | Train: 1.071270 | LR: 0.000701\n",
      "Epoch [ 64/100] | Mini-Batch [1536/2345] | Train: 1.048637 | LR: 0.000691\n",
      "Epoch [ 64/100] | Validation: 1.040465\n",
      "Epoch [ 65/100] | Mini-Batch [ 512/2345] | Train: 1.052650 | LR: 0.000677\n",
      "Epoch [ 65/100] | Mini-Batch [1024/2345] | Train: 1.066707 | LR: 0.000668\n",
      "Epoch [ 65/100] | Mini-Batch [1536/2345] | Train: 1.066262 | LR: 0.000658\n",
      "Epoch [ 65/100] | Validation: 1.041234\n",
      "Epoch [ 66/100] | Mini-Batch [ 512/2345] | Train: 1.057990 | LR: 0.000644\n",
      "Epoch [ 66/100] | Mini-Batch [1024/2345] | Train: 1.054931 | LR: 0.000635\n",
      "Epoch [ 66/100] | Mini-Batch [1536/2345] | Train: 1.051746 | LR: 0.000626\n",
      "Epoch [ 66/100] | Validation: 1.039812\n",
      "Epoch [ 67/100] | Mini-Batch [ 512/2345] | Train: 1.058616 | LR: 0.000612\n",
      "Epoch [ 67/100] | Mini-Batch [1024/2345] | Train: 1.066651 | LR: 0.000603\n",
      "Epoch [ 67/100] | Mini-Batch [1536/2345] | Train: 1.057899 | LR: 0.000593\n",
      "Epoch [ 67/100] | Validation: 1.039104\n",
      "Epoch [ 68/100] | Mini-Batch [ 512/2345] | Train: 1.062397 | LR: 0.000580\n",
      "Epoch [ 68/100] | Mini-Batch [1024/2345] | Train: 1.072444 | LR: 0.000571\n",
      "Epoch [ 68/100] | Mini-Batch [1536/2345] | Train: 1.065387 | LR: 0.000562\n",
      "Epoch [ 68/100] | Validation: 1.039221\n",
      "Epoch [ 69/100] | Mini-Batch [ 512/2345] | Train: 1.052647 | LR: 0.000548\n",
      "Epoch [ 69/100] | Mini-Batch [1024/2345] | Train: 1.062412 | LR: 0.000540\n",
      "Epoch [ 69/100] | Mini-Batch [1536/2345] | Train: 1.059512 | LR: 0.000531\n",
      "Epoch [ 69/100] | Validation: 1.037890\n",
      "Epoch [ 70/100] | Mini-Batch [ 512/2345] | Train: 1.077711 | LR: 0.000518\n",
      "Epoch [ 70/100] | Mini-Batch [1024/2345] | Train: 1.061109 | LR: 0.000509\n",
      "Epoch [ 70/100] | Mini-Batch [1536/2345] | Train: 1.055536 | LR: 0.000500\n",
      "Epoch [ 70/100] | Validation: 1.037664\n",
      "Epoch [ 71/100] | Mini-Batch [ 512/2345] | Train: 1.065407 | LR: 0.000487\n",
      "Epoch [ 71/100] | Mini-Batch [1024/2345] | Train: 1.053792 | LR: 0.000479\n",
      "Epoch [ 71/100] | Mini-Batch [1536/2345] | Train: 1.060338 | LR: 0.000470\n",
      "Epoch [ 71/100] | Validation: 1.037659\n",
      "Epoch [ 72/100] | Mini-Batch [ 512/2345] | Train: 1.066596 | LR: 0.000458\n",
      "Epoch [ 72/100] | Mini-Batch [1024/2345] | Train: 1.054349 | LR: 0.000449\n",
      "Epoch [ 72/100] | Mini-Batch [1536/2345] | Train: 1.056286 | LR: 0.000441\n",
      "Epoch [ 72/100] | Validation: 1.036814\n",
      "Epoch [ 73/100] | Mini-Batch [ 512/2345] | Train: 1.056162 | LR: 0.000429\n",
      "Epoch [ 73/100] | Mini-Batch [1024/2345] | Train: 1.058986 | LR: 0.000420\n",
      "Epoch [ 73/100] | Mini-Batch [1536/2345] | Train: 1.066507 | LR: 0.000412\n",
      "Epoch [ 73/100] | Validation: 1.036861\n",
      "Epoch [ 74/100] | Mini-Batch [ 512/2345] | Train: 1.057175 | LR: 0.000400\n",
      "Epoch [ 74/100] | Mini-Batch [1024/2345] | Train: 1.053288 | LR: 0.000392\n",
      "Epoch [ 74/100] | Mini-Batch [1536/2345] | Train: 1.059145 | LR: 0.000385\n",
      "Epoch [ 74/100] | Validation: 1.036376\n",
      "Epoch [ 75/100] | Mini-Batch [ 512/2345] | Train: 1.060644 | LR: 0.000373\n",
      "Epoch [ 75/100] | Mini-Batch [1024/2345] | Train: 1.049566 | LR: 0.000365\n",
      "Epoch [ 75/100] | Mini-Batch [1536/2345] | Train: 1.060418 | LR: 0.000357\n",
      "Epoch [ 75/100] | Validation: 1.035554\n",
      "Epoch [ 76/100] | Mini-Batch [ 512/2345] | Train: 1.053736 | LR: 0.000346\n",
      "Epoch [ 76/100] | Mini-Batch [1024/2345] | Train: 1.065982 | LR: 0.000338\n",
      "Epoch [ 76/100] | Mini-Batch [1536/2345] | Train: 1.055512 | LR: 0.000331\n",
      "Epoch [ 76/100] | Validation: 1.035768\n",
      "Epoch [ 77/100] | Mini-Batch [ 512/2345] | Train: 1.068806 | LR: 0.000320\n",
      "Epoch [ 77/100] | Mini-Batch [1024/2345] | Train: 1.057936 | LR: 0.000313\n",
      "Epoch [ 77/100] | Mini-Batch [1536/2345] | Train: 1.050413 | LR: 0.000306\n",
      "Epoch [ 77/100] | Validation: 1.035357\n",
      "Epoch [ 78/100] | Mini-Batch [ 512/2345] | Train: 1.044607 | LR: 0.000295\n",
      "Epoch [ 78/100] | Mini-Batch [1024/2345] | Train: 1.070847 | LR: 0.000288\n",
      "Epoch [ 78/100] | Mini-Batch [1536/2345] | Train: 1.052603 | LR: 0.000281\n",
      "Epoch [ 78/100] | Validation: 1.035495\n",
      "Epoch [ 79/100] | Mini-Batch [ 512/2345] | Train: 1.065319 | LR: 0.000271\n",
      "Epoch [ 79/100] | Mini-Batch [1024/2345] | Train: 1.047148 | LR: 0.000264\n",
      "Epoch [ 79/100] | Mini-Batch [1536/2345] | Train: 1.045742 | LR: 0.000257\n",
      "Epoch [ 79/100] | Validation: 1.035057\n",
      "Epoch [ 80/100] | Mini-Batch [ 512/2345] | Train: 1.051464 | LR: 0.000247\n",
      "Epoch [ 80/100] | Mini-Batch [1024/2345] | Train: 1.058735 | LR: 0.000241\n",
      "Epoch [ 80/100] | Mini-Batch [1536/2345] | Train: 1.051470 | LR: 0.000234\n",
      "Epoch [ 80/100] | Validation: 1.034648\n",
      "Epoch [ 81/100] | Mini-Batch [ 512/2345] | Train: 1.056579 | LR: 0.000225\n",
      "Epoch [ 81/100] | Mini-Batch [1024/2345] | Train: 1.056757 | LR: 0.000218\n",
      "Epoch [ 81/100] | Mini-Batch [1536/2345] | Train: 1.055374 | LR: 0.000212\n",
      "Epoch [ 81/100] | Validation: 1.034660\n",
      "Epoch [ 82/100] | Mini-Batch [ 512/2345] | Train: 1.055091 | LR: 0.000203\n",
      "Epoch [ 82/100] | Mini-Batch [1024/2345] | Train: 1.045267 | LR: 0.000197\n",
      "Epoch [ 82/100] | Mini-Batch [1536/2345] | Train: 1.063739 | LR: 0.000191\n",
      "Epoch [ 82/100] | Validation: 1.034473\n",
      "Epoch [ 83/100] | Mini-Batch [ 512/2345] | Train: 1.067156 | LR: 0.000182\n",
      "Epoch [ 83/100] | Mini-Batch [1024/2345] | Train: 1.047322 | LR: 0.000177\n",
      "Epoch [ 83/100] | Mini-Batch [1536/2345] | Train: 1.059194 | LR: 0.000171\n",
      "Epoch [ 83/100] | Validation: 1.034409\n",
      "Epoch [ 84/100] | Mini-Batch [ 512/2345] | Train: 1.056584 | LR: 0.000163\n",
      "Epoch [ 84/100] | Mini-Batch [1024/2345] | Train: 1.051591 | LR: 0.000157\n",
      "Epoch [ 84/100] | Mini-Batch [1536/2345] | Train: 1.068153 | LR: 0.000152\n",
      "Epoch [ 84/100] | Validation: 1.034152\n",
      "Epoch [ 85/100] | Mini-Batch [ 512/2345] | Train: 1.065193 | LR: 0.000144\n",
      "Epoch [ 85/100] | Mini-Batch [1024/2345] | Train: 1.061751 | LR: 0.000139\n",
      "Epoch [ 85/100] | Mini-Batch [1536/2345] | Train: 1.049689 | LR: 0.000134\n",
      "Epoch [ 85/100] | Validation: 1.034228\n",
      "Epoch [ 86/100] | Mini-Batch [ 512/2345] | Train: 1.060077 | LR: 0.000127\n",
      "Epoch [ 86/100] | Mini-Batch [1024/2345] | Train: 1.049879 | LR: 0.000122\n",
      "Epoch [ 86/100] | Mini-Batch [1536/2345] | Train: 1.053326 | LR: 0.000117\n",
      "Epoch [ 86/100] | Validation: 1.034276\n",
      "Epoch [ 87/100] | Mini-Batch [ 512/2345] | Train: 1.061357 | LR: 0.000110\n",
      "Epoch [ 87/100] | Mini-Batch [1024/2345] | Train: 1.053093 | LR: 0.000106\n",
      "Epoch [ 87/100] | Mini-Batch [1536/2345] | Train: 1.046926 | LR: 0.000101\n",
      "Epoch [ 87/100] | Validation: 1.034232\n",
      "Epoch [ 88/100] | Mini-Batch [ 512/2345] | Train: 1.053972 | LR: 0.000095\n",
      "Epoch [ 88/100] | Mini-Batch [1024/2345] | Train: 1.056216 | LR: 0.000091\n",
      "Epoch [ 88/100] | Mini-Batch [1536/2345] | Train: 1.057136 | LR: 0.000087\n",
      "Epoch [ 88/100] | Validation: 1.034132\n",
      "Epoch [ 89/100] | Mini-Batch [ 512/2345] | Train: 1.047382 | LR: 0.000081\n",
      "Epoch [ 89/100] | Mini-Batch [1024/2345] | Train: 1.066679 | LR: 0.000077\n",
      "Epoch [ 89/100] | Mini-Batch [1536/2345] | Train: 1.056310 | LR: 0.000073\n",
      "Epoch [ 89/100] | Validation: 1.034078\n",
      "Epoch [ 90/100] | Mini-Batch [ 512/2345] | Train: 1.053999 | LR: 0.000068\n",
      "Epoch [ 90/100] | Mini-Batch [1024/2345] | Train: 1.062153 | LR: 0.000064\n",
      "Epoch [ 90/100] | Mini-Batch [1536/2345] | Train: 1.048228 | LR: 0.000061\n",
      "Epoch [ 90/100] | Validation: 1.034183\n",
      "Epoch [ 91/100] | Mini-Batch [ 512/2345] | Train: 1.058344 | LR: 0.000055\n",
      "Epoch [ 91/100] | Mini-Batch [1024/2345] | Train: 1.048373 | LR: 0.000052\n",
      "Epoch [ 91/100] | Mini-Batch [1536/2345] | Train: 1.058841 | LR: 0.000049\n",
      "Epoch [ 91/100] | Validation: 1.034084\n",
      "Epoch [ 92/100] | Mini-Batch [ 512/2345] | Train: 1.060050 | LR: 0.000045\n",
      "Epoch [ 92/100] | Mini-Batch [1024/2345] | Train: 1.053295 | LR: 0.000042\n",
      "Epoch [ 92/100] | Mini-Batch [1536/2345] | Train: 1.051345 | LR: 0.000039\n",
      "Epoch [ 92/100] | Validation: 1.034119\n",
      "Epoch [ 93/100] | Mini-Batch [ 512/2345] | Train: 1.044103 | LR: 0.000035\n",
      "Epoch [ 93/100] | Mini-Batch [1024/2345] | Train: 1.073145 | LR: 0.000032\n",
      "Epoch [ 93/100] | Mini-Batch [1536/2345] | Train: 1.061905 | LR: 0.000030\n",
      "Epoch [ 93/100] | Validation: 1.034113\n",
      "Epoch [ 94/100] | Mini-Batch [ 512/2345] | Train: 1.059415 | LR: 0.000026\n",
      "Epoch [ 94/100] | Mini-Batch [1024/2345] | Train: 1.048651 | LR: 0.000024\n",
      "Epoch [ 94/100] | Mini-Batch [1536/2345] | Train: 1.062823 | LR: 0.000022\n",
      "Epoch [ 94/100] | Validation: 1.034105\n",
      "Epoch [ 95/100] | Mini-Batch [ 512/2345] | Train: 1.055917 | LR: 0.000019\n",
      "Epoch [ 95/100] | Mini-Batch [1024/2345] | Train: 1.058872 | LR: 0.000017\n",
      "Epoch [ 95/100] | Mini-Batch [1536/2345] | Train: 1.059072 | LR: 0.000015\n",
      "Epoch [ 95/100] | Validation: 1.034226\n",
      "Epoch [ 96/100] | Mini-Batch [ 512/2345] | Train: 1.061525 | LR: 0.000013\n",
      "Epoch [ 96/100] | Mini-Batch [1024/2345] | Train: 1.054026 | LR: 0.000011\n",
      "Epoch [ 96/100] | Mini-Batch [1536/2345] | Train: 1.054371 | LR: 0.000010\n",
      "Epoch [ 96/100] | Validation: 1.034185\n",
      "Epoch [ 97/100] | Mini-Batch [ 512/2345] | Train: 1.074042 | LR: 0.000008\n",
      "Epoch [ 97/100] | Mini-Batch [1024/2345] | Train: 1.045264 | LR: 0.000007\n",
      "Epoch [ 97/100] | Mini-Batch [1536/2345] | Train: 1.050503 | LR: 0.000006\n",
      "Epoch [ 97/100] | Validation: 1.034152\n",
      "Epoch [ 98/100] | Mini-Batch [ 512/2345] | Train: 1.047053 | LR: 0.000004\n",
      "Epoch [ 98/100] | Mini-Batch [1024/2345] | Train: 1.063286 | LR: 0.000003\n",
      "Epoch [ 98/100] | Mini-Batch [1536/2345] | Train: 1.066354 | LR: 0.000003\n",
      "Epoch [ 98/100] | Validation: 1.034151\n",
      "Epoch [ 99/100] | Mini-Batch [ 512/2345] | Train: 1.063618 | LR: 0.000002\n",
      "Epoch [ 99/100] | Mini-Batch [1024/2345] | Train: 1.042792 | LR: 0.000001\n",
      "Epoch [ 99/100] | Mini-Batch [1536/2345] | Train: 1.056902 | LR: 0.000001\n",
      "Epoch [ 99/100] | Validation: 1.034118\n",
      "Epoch [100/100] | Mini-Batch [ 512/2345] | Train: 1.053588 | LR: 0.000000\n",
      "Epoch [100/100] | Mini-Batch [1024/2345] | Train: 1.055133 | LR: 0.000000\n",
      "Epoch [100/100] | Mini-Batch [1536/2345] | Train: 1.063327 | LR: 0.000000\n",
      "Epoch [100/100] | Validation: 1.034116\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') # Initialize as highest possible\n",
    "\n",
    "print(f'Effective Batch Size: {batch_size * accum_steps} examples')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        with torch.amp.autocast(device_type=str(device)):\n",
    "            outputs = model(inputs)\n",
    "            mse_loss = criterion_mse(outputs, labels)\n",
    "            spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "            # Normalize mini losses to emulate larger batch size (not just accumulation)\n",
    "            loss = (mse_loss + spectral_weight * spec_loss) / accum_steps\n",
    "        loss.backward() # Accumulate gradients\n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN detected in loss at epoch {epoch+1}, batch {i+1}\")\n",
    "            print(f\"MSE Loss: {mse_loss.item()}, Spectral Loss: {spec_loss.item()}\")\n",
    "        # Showing the true mini-batch loss during logging\n",
    "        running_loss += loss.item() * accum_steps\n",
    "        if str(device) == \"cuda\":\n",
    "            del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        if i % (2 * accum_steps) == (2 * accum_steps) - 1:\n",
    "            print(f'Epoch [{epoch+1:3}/{num_epochs}] | '\n",
    "                  f'Mini-Batch [{i+1:4}/{len(data_loader)}] | '\n",
    "                  f'Train: {(running_loss / 500):8.6f} | '\n",
    "                  f'LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')\n",
    "            running_loss = 0.0\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                outputs = model(inputs)\n",
    "                mse_loss = criterion_mse(outputs, labels)\n",
    "                spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "                loss = mse_loss + spectral_weight * spec_loss\n",
    "                val_loss += loss.item()\n",
    "            if device.type == \"cuda\":\n",
    "                del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch [{epoch+1:3}/{num_epochs}] | Validation: {avg_val_loss:8.6f}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, 'best_audio_unet.pth')\n",
    "    if str(device) == \"cuda\":\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated audio saved at: generated_audio.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0894,  0.3314,  0.0275],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0844, -0.0415, -0.0415]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "        state_dict = state_dict['model_state_dict']\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if 'weight_mask' not in key:\n",
    "            new_key = key.replace('weight_orig', 'weight')\n",
    "            new_state_dict[new_key] = value\n",
    "    model = AudioUNet(num_channels=2).to(device).float()\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_audio(audio_path, target_sample_rate=44100):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != target_sample_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = transform(waveform)\n",
    "    # Normalize the waveform\n",
    "    waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "    # Ensure the waveform is stereo\n",
    "    if waveform.shape[0] == 1:\n",
    "        waveform = waveform.repeat(2, 1)\n",
    "    elif waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def infer_audio(model, audio_path, output_path, device, chunk_duration=2, sample_rate=44100):\n",
    "    waveform = preprocess_audio(audio_path, target_sample_rate=sample_rate)\n",
    "    chunk_size = int(chunk_duration * sample_rate)\n",
    "    total_length = waveform.shape[1]\n",
    "    processed_waveform = torch.zeros_like(waveform)\n",
    "    overlap_count = torch.zeros(waveform.shape[1], dtype=torch.float32)\n",
    "    overlap = chunk_size // 2\n",
    "    window = torch.hann_window(chunk_size, dtype=torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, total_length, overlap):\n",
    "            end = min(start + chunk_size, total_length)\n",
    "            if end - start < chunk_size:\n",
    "                chunk = torch.zeros((2, chunk_size), dtype=torch.float32)\n",
    "                chunk[:, :(end - start)] = waveform[:, start:end]\n",
    "            else:\n",
    "                chunk = waveform[:, start:end]\n",
    "            chunk = chunk.unsqueeze(0).to(device)\n",
    "            output = model(chunk).squeeze(0).cpu()\n",
    "            if end - start < chunk_size:\n",
    "                valid_length = end - start\n",
    "                output = output[:, :valid_length]\n",
    "                window_section = window[:valid_length]\n",
    "            else:\n",
    "                window_section = window\n",
    "            output = output * window_section.view(1, -1)\n",
    "            if end - start < chunk_size:\n",
    "                processed_waveform[:, start:end] += output\n",
    "                overlap_count[start:end] += window_section\n",
    "            else:\n",
    "                processed_waveform[:, start:start + chunk_size] += output\n",
    "                overlap_count[start:start + chunk_size] += window_section\n",
    "    mask = overlap_count > 0\n",
    "    processed_waveform[:, mask] /= overlap_count[mask].view(1, -1)\n",
    "    processed_waveform = processed_waveform / (torch.max(torch.abs(processed_waveform)) + 1e-8)\n",
    "    processed_waveform = torch.clamp(processed_waveform, -0.99, 0.99)\n",
    "    sf.write(output_path, processed_waveform.T.numpy(), sample_rate)\n",
    "    print(f\"Generated audio saved at: {output_path}\")\n",
    "    return processed_waveform\n",
    "\n",
    "model_path  = \"best_audio_unet.pth\"\n",
    "audio_path  = \"input_audio.wav\"\n",
    "output_path = \"generated_audio.wav\"\n",
    "\n",
    "# You'll need more than 16GB of GPU memory to run via CUDA\n",
    "device = torch.device(\"cpu\")\n",
    "model = load_model(model_path, device)\n",
    "infer_audio(model, audio_path, output_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
