{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing - Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple\n",
    "from scipy.io.wavfile import write\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:\\t\\t\\t{device}\")\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Allocated CUDA memory:\\t{torch.cuda.memory_allocated() / 1024 ** 3:8.4f} GiB\")\n",
    "\n",
    "num_epochs = 75         # Number of epochs to train\n",
    "data_dir = \"/mnt/data/Daftset\" # Directory containing the dataset (we will work on a copy)\n",
    "batch_size = 1          # 3060 tackles single-entry batches @ 2 sec each at max\n",
    "learning_rate = 1e-3    # Light learning rate\n",
    "num_channels = 2        # Number of audio channels\n",
    "freq_orig = 44100       # Original frequency of the audio files\n",
    "optim_k = 5             # Average weight updates over optim_k steps to stabilize training\n",
    "optim_alpha = 0.3       # Weight of influence of Lookahead's fast weights on the slow weights\n",
    "chunk_duration = 2      # Duration of training examples in seconds -> samples_per_example = (freq_orig) * chunk_duration\n",
    "weight_decay = 1e-4     # Weight decay for the optimizer\n",
    "spectral_weight = 0.75  # Spectral loss impact for total loss calculation\n",
    "mse_weight = 0.25       # Impact weight for MSE to total loss calculation\n",
    "accum_steps = 8         # Effective_batch_size = batch_size * accumulation_steps (too large and the model might aim to remove an \"average drum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller datasets may afford us to load the entire dataset at once during `init`, but this set is custom and sizewise unpredictable enough to require on-the-fly loading.\n",
    "Thing is, if we strictly load on request, how can we provide a `__len__` method for the dataset?<br>\n",
    "Thankfully, audio files have metadata we can use to determine their length without loading them, based on which we can provide a `__len__` method.\n",
    "\n",
    "- `_process_files` loads the in-out file pairs by name and as a list of name pairs, but not the actual audio data. We need that for ordered access to the dataset.\n",
    "- `_get_file_info` takes this list of name pairs and loads just the metadata of the audio files, which we can use to determine the length of all files. Note that we expect input and output to be the same size here. This is unrealistic, but we can trim or pad the labels accordingly later when we actually load the data.\n",
    "- The main data providing complexity is in `__get_item__`, where we \n",
    "    - calculate the idx of the requested input, label pair at a global scale (across all audio)\n",
    "    - assemble the input and the label pair, even across different files (load until we reach the requested sample size)\n",
    "    - trim or pad the label to match the input size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, batch_size: int = batch_size, chunk_duration: float = chunk_duration, \n",
    "                 freq_orig: int = freq_orig, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_tail = input_tail\n",
    "        self.label_tail = label_tail\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.freq_orig = freq_orig\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)  # Get input-label file name pairs \n",
    "        self.file_info = self._get_file_info() # Sift only through metadata for each file\n",
    "        self.input_length = sum(info['length'] for info in self.file_info) # Calculate total length and chunk information (from metadata)\n",
    "        self.chunk_size = int(chunk_duration * freq_orig)  # Ensure chunk_size is an integer\n",
    "        self.batch_count = 0\n",
    "        self._file_handle_cache = {} # Create cache for file handles\n",
    "\n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> List[Tuple[str, str]]:\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        file_tuples = []\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        \n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _get_file_info(self) -> List[dict]:\n",
    "        file_info = []\n",
    "        for in_fname, lb_fname in self.input_label_pairs:\n",
    "            info = torchaudio.info(os.path.join(self.data_dir, in_fname))\n",
    "            length = info.num_frames\n",
    "            file_info.append({'length': length,\n",
    "                              'input_path': os.path.join(self.data_dir, in_fname),\n",
    "                              'label_path': os.path.join(self.data_dir, lb_fname)})\n",
    "        return file_info\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def _get_file_handle(self, file_path: str) -> sf.SoundFile:\n",
    "        if file_path not in self._file_handle_cache:\n",
    "            self._file_handle_cache[file_path] = sf.SoundFile(file_path, 'r')\n",
    "        return self._file_handle_cache[file_path]\n",
    "\n",
    "    def _normalize_audio(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        data = data.float()\n",
    "        if torch.abs(data).max() > 0:\n",
    "            data = data / (torch.abs(data).max() + 1e-8)\n",
    "        return torch.clamp(data, min=-0.99, max=0.99)  # Prevent extreme values\n",
    "\n",
    "    def _pad_audio(self, audio: torch.Tensor, target_length: int) -> torch.Tensor:\n",
    "        current_length = audio.shape[1]\n",
    "        if current_length < target_length:\n",
    "            padding = target_length - current_length\n",
    "            return F.pad(audio, (0, padding))\n",
    "        elif current_length > target_length:\n",
    "            return audio[:, :target_length]\n",
    "        return audio\n",
    "\n",
    "    def _read_audio_chunk(self, file_path: str, start: int, length: int) -> torch.Tensor:\n",
    "        handle = self._get_file_handle(file_path)\n",
    "        handle.seek(start)\n",
    "        data = handle.read(length)\n",
    "        data = torch.from_numpy(data).T\n",
    "        data = self._normalize_audio(data)\n",
    "        return self._pad_audio(data, self.chunk_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        total_chunks = self.input_length // self.chunk_size\n",
    "        return total_chunks // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "            \n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunks, label_chunks = [], []\n",
    "        samples_remaining = self.chunk_size\n",
    "        cumulative_length = 0\n",
    "        current_file_index = 0\n",
    "\n",
    "        # First, find the correct starting file\n",
    "        while cumulative_length + self.file_info[current_file_index]['length'] <= global_start:\n",
    "            cumulative_length += self.file_info[current_file_index]['length']\n",
    "            current_file_index += 1\n",
    "            if current_file_index >= len(self.file_info):\n",
    "                current_file_index = 0\n",
    "                cumulative_length = 0\n",
    "\n",
    "        # Now read the chunks\n",
    "        while samples_remaining > 0:\n",
    "            if current_file_index >= len(self.file_info):\n",
    "                current_file_index = 0\n",
    "\n",
    "            file_info = self.file_info[current_file_index]\n",
    "            local_start = global_start - cumulative_length if current_file_index == 0 else 0\n",
    "            \n",
    "            # Calculate how many samples we can take from this file\n",
    "            samples_from_file = min(\n",
    "                file_info['length'] - local_start,  # samples available in file\n",
    "                samples_remaining  # samples we still need\n",
    "            )\n",
    "\n",
    "            if samples_from_file > 0:\n",
    "                input_chunk = self._read_audio_chunk(\n",
    "                    file_info['input_path'], \n",
    "                    local_start, \n",
    "                    samples_from_file\n",
    "                )\n",
    "                label_chunk = self._read_audio_chunk(\n",
    "                    file_info['label_path'], \n",
    "                    local_start, \n",
    "                    samples_from_file\n",
    "                )\n",
    "                \n",
    "                input_chunks.append(input_chunk)\n",
    "                label_chunks.append(label_chunk)\n",
    "                samples_remaining -= samples_from_file\n",
    "\n",
    "            cumulative_length += file_info['length']\n",
    "            current_file_index += 1\n",
    "\n",
    "        # Concatenate all chunks and ensure final size\n",
    "        input_audio = torch.cat(input_chunks, dim=1)\n",
    "        label_audio = torch.cat(label_chunks, dim=1)\n",
    "\n",
    "        # Final size check and adjustment\n",
    "        input_audio = self._pad_audio(input_audio, self.chunk_size)\n",
    "        label_audio = self._pad_audio(label_audio, self.chunk_size)\n",
    "\n",
    "        return input_audio, label_audio\n",
    "\n",
    "    def get_batch(self, batch_size: int = batch_size, randomized: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if randomized:\n",
    "            idx = np.random.randint(0, len(self))\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "                \n",
    "        batch = [self.__getitem__((idx + i) % len(self)) for i in range(batch_size)]\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        \n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "            \n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    def __del__(self):\n",
    "        for handle in self._file_handle_cache.values():\n",
    "            handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Call Sanity Check\n",
    "input, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "# Expect shape: (2, assert_f) -> stereo and [chunk duration] sample count\n",
    "assert input.shape == (2, assert_f), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, assert_f), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch()\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: (batch, 2, assert_f)\n",
    "assert input_batch.shape == (batch_size, 2, assert_f), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (batch_size, 2, assert_f), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: (batch, 2, assert_f)\n",
    "assert input_collate.shape == (batch_size, 2, assert_f), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (batch_size, 2, assert_f), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "if batch_size == 1:\n",
    "    assert torch.equal(input, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input, label_audio = dataset[i]\n",
    "    assert input.shape == (2, assert_f), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch()\n",
    "    assert input_batch.shape == (batch_size, 2, assert_f), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (batch_size, 2, assert_f), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, assert_f), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, assert_f), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(randomized=True)\n",
    "    assert input_batch.shape == (batch_size, 2, assert_f), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (batch_size, 2, assert_f), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check dataset item size consistency\n",
    "for idx in tqdm(range(len(dataset)), desc=\"Checking dataset item size consistency\"):\n",
    "    input, label_audio = dataset[idx]\n",
    "    assert input.shape == (2, assert_f), f\"Error at index {idx}: Input tensor shape mismatch. Expected (2, {assert_f}), got {input.shape}\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {idx}: Label tensor shape mismatch. Expected (2, {assert_f}), got {label_audio.shape}\"\n",
    "\n",
    "# Taking a listen ensures consistent pair assembly\n",
    "input, label_audio = dataset[np.random.randint(0, len(dataset)-1)]\n",
    "input = input.T.numpy()\n",
    "label_audio = label_audio.T.numpy()\n",
    "write('input_sample.wav', freq_orig, input)\n",
    "write('label_sample.wav', freq_orig, label_audio)\n",
    "print(\"Wrote input_sample.wav and label_sample.wav for inspection.\")\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=32, num_layers=6):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_channels + i * growth_rate\n",
    "            self.norms.append(nn.InstanceNorm1d(in_ch))\n",
    "            self.layers.append(nn.Sequential(nn.Conv1d(in_ch, growth_rate, kernel_size=3, padding=1),\n",
    "                                             nn.PReLU(),\n",
    "                                             nn.Dropout(0.1)))\n",
    "        final_ch = in_channels + num_layers * growth_rate\n",
    "        self.final_norm = nn.InstanceNorm1d(final_ch)\n",
    "        self.final_conv = nn.Conv1d(final_ch, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for norm, layer in zip(self.norms, self.layers):\n",
    "            inputs = torch.cat(features, dim=1)\n",
    "            inputs = norm(inputs)\n",
    "            out = layer(inputs)\n",
    "            features.append(out)\n",
    "        out = self.final_norm(torch.cat(features, dim=1))\n",
    "        out = self.final_conv(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class ComplexSTFTLayer(nn.Module):\n",
    "    # Related to loss calculation, not model architecture\n",
    "    def __init__(self, n_fft=2048, hop_length=512, win_length=None, window=None):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length or n_fft\n",
    "        self.window = window or torch.hann_window(self.win_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        window = self.window.to(device)\n",
    "        stft = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                         win_length=self.win_length, window=window,\n",
    "                         return_complex=True)\n",
    "        mag = torch.abs(stft)\n",
    "        phase = torch.angle(stft)\n",
    "        return mag, phase, stft\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.PReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Resizer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, target_size):\n",
    "        super(Resizer, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channel, out_channel, kernel_size=4, stride=2, padding=1)\n",
    "        self.target_size = target_size\n",
    "        self.activation = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=self.target_size, mode='linear', align_corners=True)\n",
    "        return self.activation(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "                                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                nn.PReLU())\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        diff = x.size(2) - skip.size(2)\n",
    "        if diff > 0:\n",
    "            x = x[:, :, :skip.size(2)]\n",
    "        elif diff < 0:\n",
    "            x = nn.functional.pad(x, (0, -diff))\n",
    "        return x\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=14):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        dilation_rates = [2**i for i in range(4)] * (num_blocks // 4) # Cyclic dilated convolutions\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels, dilation_rates) for dilation_rates in dilation_rates])\n",
    "            \n",
    "    def _build_block(self, in_channels, out_channels, dilation):\n",
    "        return nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),\n",
    "                             nn.PReLU(),\n",
    "                             nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "                             nn.PReLU(),\n",
    "                             nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=0.1)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        x = self.norm2(x + self.feed_forward(x))\n",
    "        return x.permute(1, 2, 0)\n",
    "\n",
    "class WaveformContextEncoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.context_net = nn.Sequential(nn.Conv1d(channels, channels*2, 15, padding=7, groups=channels),\n",
    "                                         nn.PReLU(),\n",
    "                                         nn.Conv1d(channels*2, channels*2, 1),\n",
    "                                         nn.PReLU(),\n",
    "                                         nn.Conv1d(channels*2, channels, 1),\n",
    "                                         nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        context = self.context_net(x)\n",
    "        return x * context\n",
    "\n",
    "class MultiScaleProcessing(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.scales = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv1d(channels, channels, 2**i + 1, padding=2**(i-1)), nn.PReLU()) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = [scale(x) for scale in self.scales]\n",
    "        return sum(outputs) / len(outputs)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.embd1 = WaveformContextEncoder(num_channels)\n",
    "        self.down2 = DownSample(num_channels, 4)\n",
    "        self.down3 = DownSample(4, 8)\n",
    "        self.down4 = DownSample(8, 16)\n",
    "        self.cv_tasnet = CV_TasNet_Block(16, 16)\n",
    "        self.temporal_attention = TemporalSelfAttention(16)\n",
    "        self.up4 = UpSample(16, 8)\n",
    "        self.up3 = UpSample(8, 4)\n",
    "        self.up2 = UpSample(4, num_channels)\n",
    "        self.resizer = Resizer(num_channels, num_channels, assert_f)\n",
    "        self.up1 = nn.Sequential(nn.Conv1d(4, 4, kernel_size=3, padding=1),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(4, num_channels, kernel_size=3, padding=1),\n",
    "                                 nn.Tanh()) # Normalize to [-1, 1] for audio\n",
    " \n",
    "    def forward(self, x):\n",
    "        x_orig = x\n",
    "        skip1 = self.down1(x)            # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        skip1 = self.embd1(skip1)        # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        skip2 = self.down2(skip1)        # (batch, 4, ((freq_orig) * chunk_duration) / 2)\n",
    "        skip3 = self.down3(skip2)        # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        skip4 = self.down4(skip3)        # (batch, 16, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.cv_tasnet(skip4)        # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.temporal_attention(x)   # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.up4(x, skip4)           # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up3(x, skip3)           # (batch, 4, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up2(x, skip2)           # (batch, 2, ((freq_orig) * chunk_duration) / 2)\n",
    "        x = self.resizer(x)              # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        x = torch.cat([x, skip1], dim=1) # (batch, 4, (freq_orig) * chunk_duration)\n",
    "        x = self.up1(x)                  # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        return x + x_orig                # x applies 'morphing' to original input audio -> model focuses solely on splicing, not reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_splits(dataset, val_ratio=0.2, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_ratio * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    return SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "train_sampler, valid_sampler = create_train_val_splits(dataset, val_ratio=0.2, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)\n",
    "valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, input, n_fft=2048, hop_length=512, eps=1e-8):\n",
    "    # STFT layer for magnitude spectrogram calculation\n",
    "    stft_layer = ComplexSTFTLayer(n_fft=n_fft, hop_length=hop_length).to(device)\n",
    "    # Reshape to (batch * channels, time)\n",
    "    batch_size, channels, samples = output.shape\n",
    "    out_2d = output.reshape(batch_size * channels, samples)\n",
    "    tgt_2d = target.reshape(batch_size * channels, samples)\n",
    "    inp_2d = input.reshape(batch_size * channels, samples)\n",
    "    # Magnitude spectrograms\n",
    "    magn_out = torch.abs(stft_layer(out_2d)[0]) \n",
    "    magn_tar = torch.abs(stft_layer(tgt_2d)[0])\n",
    "    magn_inp = torch.abs(stft_layer(inp_2d)[0])\n",
    "    # Phase spectrograms\n",
    "    spec_out = stft_layer(out_2d)[2]\n",
    "    spec_tar = stft_layer(tgt_2d)[2]\n",
    "    spec_out_stable = torch.where(torch.abs(spec_out) > eps, spec_out, eps + 0j)\n",
    "    spec_tar_stable = torch.where(torch.abs(spec_tar) > eps, spec_tar, eps + 0j)\n",
    "    phase_out = torch.angle(spec_out_stable)\n",
    "    phase_tar = torch.angle(spec_tar_stable)\n",
    "    # Reshape back to (batch, channels, freq, time), Convert to log scale\n",
    "    magn_out_db = torch.log1p(torch.clamp(magn_out.reshape(batch_size, channels, magn_out.shape[1], magn_out.shape[2]), min=eps))\n",
    "    magn_tar_db = torch.log1p(torch.clamp(magn_tar.reshape(batch_size, channels, magn_tar.shape[1], magn_tar.shape[2]), min=eps))\n",
    "    magn_inp_db = torch.log1p(torch.clamp(magn_inp.reshape(batch_size, channels, magn_inp.shape[1], magn_inp.shape[2]), min=eps))\n",
    "    # Resolution/width of each frequency bin in Hz\n",
    "    freq_bin_res = freq_orig / n_fft\n",
    "    # Kick drum range\n",
    "    kick_min, kick_max = 20, 170 # Hz\n",
    "    kick_low_bin, kick_high_bin = max(1, int(kick_min / freq_bin_res)), int(kick_max / freq_bin_res)\n",
    "    # Snare drum range\n",
    "    snare_min, snare_max = 170, 600 # Hz\n",
    "    snare_low_bin, snare_high_bin = int(snare_min / freq_bin_res), int(snare_max / freq_bin_res)\n",
    "    # Cymbals range\n",
    "    cymbal_min, cymbal_max = 2000, 20000 # Hz\n",
    "    cymbal_low_bin, cymbal_high_bin = int(cymbal_min / freq_bin_res), min(n_fft // 2, int(cymbal_max / freq_bin_res))\n",
    "    # Create drum and non-drum frequency masks\n",
    "    drum_mask = torch.zeros_like(magn_out_db, device=device)\n",
    "    drum_mask[:, :, kick_low_bin:kick_high_bin, :] = 1.0\n",
    "    drum_mask[:, :, snare_low_bin:snare_high_bin, :] = 1.0\n",
    "    drum_mask[:, :, cymbal_low_bin:cymbal_high_bin, :] = 1.0\n",
    "    non_drum_mask = 1.0 - drum_mask\n",
    "    # Detect transients in the spectrograms\n",
    "    transients = torch.abs(torch.diff(magn_out_db.mean(dim=2), dim=-1))\n",
    "    # Pad the transient mask to match original length\n",
    "    transient_mask = F.pad(transients, (0, 1), mode='replicate')  # Add one frame at the end\n",
    "    transient_mask = (transient_mask > transient_mask.mean() + transient_mask.std()).float()\n",
    "    # Expand dimensions to match magn_out_db\n",
    "    transient_mask = transient_mask.unsqueeze(2).expand_as(magn_out_db)\n",
    "    # Loss 1: Remaining drum frequencies in output\n",
    "    drum_removal_loss = torch.mean(drum_mask * (1 + transient_mask) * torch.abs(magn_out_db - magn_tar_db))\n",
    "    # Loss 2: Non-drum audio signal preservation\n",
    "    content_preservation_loss = torch.mean(non_drum_mask * torch.abs(magn_out_db - magn_inp_db))\n",
    "    # Loss 3: Ensure inter-frequency band smoothness / prevent artifacts\n",
    "    smoothness_loss = torch.mean(torch.abs(magn_out_db[:, :, 1:, :] - magn_out_db[:, :, :-1, :]))\n",
    "    # Loss 4: Time domain loss\n",
    "    time_domain_loss = nn.MSELoss()(output, target)\n",
    "    # Loss 5: Phase loss - stabilized\n",
    "    phase_loss = torch.mean(1 - torch.cos((phase_out - phase_tar) % (2 * np.pi)))\n",
    "    # Loss 6: Spectral loss - stabilized with magnitude weighting\n",
    "    # Weight the complex loss by magnitude to reduce impact of noise in low-energy regions\n",
    "    magnitude_weight = (torch.abs(spec_out) + torch.abs(spec_tar)) / 2\n",
    "    magnitude_weight = torch.clamp(magnitude_weight / (torch.max(magnitude_weight) + eps), min=eps)\n",
    "    complex_spec_loss = torch.mean(magnitude_weight * torch.abs(spec_out - spec_tar))\n",
    "    # Combine losses with weights prioritizing drum removal and content preservation (I freestyled the weights)\n",
    "    return (0.80 * drum_removal_loss +          # Penalize remaining drum frequencies\n",
    "            0.45 * content_preservation_loss +  # Penalize if non-drum content is lost\n",
    "            0.30 * smoothness_loss +            # Penalize for frequency artifacts\n",
    "            0.20 * time_domain_loss +           # Penalize for time domain artifacts\n",
    "            0.10 * phase_loss +                 # Penalize for phase differences\n",
    "            0.15 * complex_spec_loss)           # Penalize for spectral differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioUNet(num_channels=2).to(device).float()\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=1e-8)\n",
    "# Gradually warm, then cool down LR\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader) // accum_steps,\n",
    "                       pct_start=0.1, anneal_strategy='cos', div_factor=10.0, final_div_factor=1000.0)\n",
    "\n",
    "print(f\"Model Parameter Count: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf') # Initialize as highest possible\n",
    "\n",
    "print(f'Effective Batch Size: {batch_size * accum_steps} examples')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        with torch.amp.autocast(device_type=str(device)):\n",
    "            outputs = model(inputs)\n",
    "            # Normalize mini losses to emulate larger batch size (not just accumulation)\n",
    "            loss = spectral_loss(outputs, labels, inputs) / accum_steps\n",
    "        loss.backward() # Accumulate gradients\n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN detected in loss at epoch {epoch+1}, batch {i+1}\")\n",
    "            break\n",
    "        # Showing the true mini-batch loss during logging\n",
    "        running_loss += loss.item() * accum_steps\n",
    "        if str(device) == \"cuda\":\n",
    "            del inputs, labels, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        if i % 512 == 512 - 1:\n",
    "            print(f'Epoch [{epoch+1:3}/{num_epochs}] | '\n",
    "                  f'Mini-Batch [{i+1:4}/{len(data_loader)}] | '\n",
    "                  f'Train: {(running_loss / 512):8.6f} | '\n",
    "                  f'LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')\n",
    "            running_loss = 0.0\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                outputs = model(inputs)\n",
    "                val_loss += spectral_loss(outputs, labels, inputs).item()\n",
    "            if device.type == \"cuda\":\n",
    "                del inputs, labels, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        print(f'Epoch [{epoch+1:3}/{num_epochs}] | Validation: {avg_val_loss:8.6f}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, 'best_audio_unet.pth')\n",
    "    if str(device) == \"cuda\":\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    # Recycled from MobileYOLOv3\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "        state_dict = state_dict['model_state_dict']\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if 'weight_mask' not in key:\n",
    "            new_key = key.replace('weight_orig', 'weight')\n",
    "            new_state_dict[new_key] = value\n",
    "    model = AudioUNet(num_channels=2).to(device).float()\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_audio(audio_path, target_sample_rate=44100):\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"File not found: {audio_path}\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != target_sample_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = transform(waveform)\n",
    "    # Normalize\n",
    "    waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-10)\n",
    "    # Ensure stereo\n",
    "    if waveform.shape[0] == 1:\n",
    "        waveform = waveform.repeat(2, 1)\n",
    "    elif waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    return waveform\n",
    "\n",
    "def infer_audio(model, audio_path, output_path, device, chunk_duration=2, sample_rate=44100):\n",
    "    waveform = preprocess_audio(audio_path, target_sample_rate=sample_rate)\n",
    "    chunk_size = int(chunk_duration * sample_rate)\n",
    "    total_length = waveform.shape[1]\n",
    "    processed_waveform = torch.zeros_like(waveform)\n",
    "    overlap_count = torch.zeros(waveform.shape[1], dtype=torch.float32)\n",
    "    overlap = chunk_size // 2\n",
    "    window = torch.hann_window(chunk_size, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, total_length, overlap), desc=\"Generating audio\"):\n",
    "            end = min(start + chunk_size, total_length)\n",
    "            if end - start < chunk_size:\n",
    "                chunk = torch.zeros((2, chunk_size), dtype=torch.float32)\n",
    "                chunk[:, :(end - start)] = waveform[:, start:end]\n",
    "            else:\n",
    "                chunk = waveform[:, start:end]\n",
    "            chunk = chunk.unsqueeze(0).to(device)\n",
    "            output = model(chunk).squeeze(0).cpu()\n",
    "            if end - start < chunk_size:\n",
    "                valid_length = end - start\n",
    "                output = output[:, :valid_length]\n",
    "                window_section = window[:valid_length]\n",
    "            else:\n",
    "                window_section = window\n",
    "            output = output * window_section.view(1, -1)\n",
    "            if end - start < chunk_size:\n",
    "                processed_waveform[:, start:end] += output\n",
    "                overlap_count[start:end] += window_section\n",
    "            else:\n",
    "                processed_waveform[:, start:start + chunk_size] += output\n",
    "                overlap_count[start:start + chunk_size] += window_section\n",
    "    mask = overlap_count > 0\n",
    "    processed_waveform[:, mask] /= overlap_count[mask].view(1, -1)\n",
    "    processed_waveform = processed_waveform / (torch.max(torch.abs(processed_waveform)) + 1e-10)\n",
    "    processed_waveform = torch.clamp(processed_waveform, -0.99, 0.99)\n",
    "    sf.write(output_path, processed_waveform.T.numpy(), sample_rate)\n",
    "    print(f\"Generated audio saved at: {output_path}\")\n",
    "    return processed_waveform\n",
    "\n",
    "model_path  = \"best_audio_unet.pth\"\n",
    "audio_path  = \"input_audio.wav\"\n",
    "output_path = \"generated_audio.wav\"\n",
    "\n",
    "# You'll need more than 16GB of GPU memory to run via CUDA\n",
    "device = torch.device(\"cpu\")\n",
    "model = load_model(model_path, device)\n",
    "infer_audio(model, audio_path, output_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
