{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing - Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:\t\t\tcuda\n",
      "Allocated CUDA memory:\t  0.0000 GiB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:\\t\\t\\t{device}\")\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(f\"Allocated CUDA memory:\\t{torch.cuda.memory_allocated() / 1024 ** 3:8.4f} GiB\")\n",
    "\n",
    "num_epochs = 10                             # Number of epochs to train\n",
    "data_dir = \"/mnt/data/Daftset/Dataset\"      # Directory containing the dataset (we will work on a copy)\n",
    "tmp_dir = \"/mnt/data/Daftset/Dataset_tmp\"   # Directory to store temporary files (aligned, resampled dataset copy)\n",
    "batch_size = 1          # 3060 tackles single-entry batches at max\n",
    "learning_rate = 5e-4    # Light learning rate\n",
    "num_channels = 2        # Number of audio channels\n",
    "freq_orig = 44100       # Original frequency of the audio files\n",
    "freq_scale = 2          # Downsampling factor\n",
    "chunk_duration = 2      # Duration of training examples in seconds -> samples_per_example = (freq_orig // freq_scale) * chunk_duration\n",
    "weight_decay = 1e-4     # Weight decay for the optimizer\n",
    "spectral_weight = 0.5   # Spectral loss impact for total loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, tmp_dir: str, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        \"\"\"Initialize the dataset with minimal memory footprint.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.tmp_dir = tmp_dir\n",
    "        self.input_tail = input_tail\n",
    "        self.label_tail = label_tail\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Process files and store only metadata\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)\n",
    "        self.file_info = self._get_file_info()\n",
    "        \n",
    "        # Calculate total length and chunk information\n",
    "        self.input_length = sum(info['length'] for info in self.file_info)\n",
    "        self.chunk_size = chunk_duration * (freq_orig // freq_scale)\n",
    "        self.batch_count = 0\n",
    "        \n",
    "        # Create cache for file handles\n",
    "        self._file_handle_cache = {}\n",
    "        \n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Process files and return input-label pairs.\"\"\"\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        \n",
    "        file_tuples = []\n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        \n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _get_file_info(self) -> List[dict]:\n",
    "        \"\"\"Get file information without loading audio data.\"\"\"\n",
    "        file_info = []\n",
    "        for in_fname, lb_fname in self.input_label_pairs:\n",
    "            info = torchaudio.info(os.path.join(self.data_dir, in_fname))\n",
    "            length = info.num_frames\n",
    "            file_info.append({\n",
    "                'length': length,\n",
    "                'input_path': os.path.join(self.data_dir, in_fname),\n",
    "                'label_path': os.path.join(self.data_dir, lb_fname)\n",
    "            })\n",
    "        return file_info\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def _get_file_handle(self, file_path: str) -> sf.SoundFile:\n",
    "        \"\"\"Get or create cached file handle.\"\"\"\n",
    "        if file_path not in self._file_handle_cache:\n",
    "            self._file_handle_cache[file_path] = sf.SoundFile(file_path, 'r')\n",
    "        return self._file_handle_cache[file_path]\n",
    "\n",
    "    def _read_audio_chunk(self, file_path: str, start: int, length: int) -> torch.Tensor:\n",
    "        \"\"\"Read a chunk of audio data efficiently.\"\"\"\n",
    "        handle = self._get_file_handle(file_path)\n",
    "        handle.seek(start)\n",
    "        data = handle.read(length)\n",
    "        return torch.from_numpy(data).T\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Calculate total number of chunks.\"\"\"\n",
    "        total_chunks = self.input_length // self.chunk_size\n",
    "        return total_chunks // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a single chunk of audio data.\"\"\"\n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunks, label_chunks = [], []\n",
    "        remaining_samples = self.chunk_size\n",
    "        cumulative_length = 0\n",
    "        \n",
    "        for file_info in self.file_info:\n",
    "            if global_start >= cumulative_length + file_info['length']:\n",
    "                cumulative_length += file_info['length']\n",
    "                continue\n",
    "                \n",
    "            local_start = global_start - cumulative_length\n",
    "            if local_start < 0:\n",
    "                local_start = 0\n",
    "                \n",
    "            samples_from_file = min(file_info['length'] - local_start, remaining_samples)\n",
    "            \n",
    "            if samples_from_file <= 0:\n",
    "                continue\n",
    "                \n",
    "            input_chunk = self._read_audio_chunk(file_info['input_path'], local_start, samples_from_file)\n",
    "            label_chunk = self._read_audio_chunk(file_info['label_path'], local_start, samples_from_file)\n",
    "            \n",
    "            input_chunks.append(input_chunk)\n",
    "            label_chunks.append(label_chunk)\n",
    "            \n",
    "            remaining_samples -= samples_from_file\n",
    "            global_start = 0\n",
    "            cumulative_length += file_info['length']\n",
    "            \n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "                \n",
    "        if remaining_samples > 0:\n",
    "            # Handle wrap-around case\n",
    "            return self.__getitem__(0)\n",
    "            \n",
    "        return (torch.cat(input_chunks, dim=1), \n",
    "                torch.cat(label_chunks, dim=1))\n",
    "\n",
    "    def get_batch(self, batch_size: int, randomized: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a batch of audio chunks.\"\"\"\n",
    "        if randomized:\n",
    "            idx = np.random.randint(0, len(self))\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "                \n",
    "        batch = [self.__getitem__((idx + i) % len(self)) for i in range(batch_size)]\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        \n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "            \n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Collate function for DataLoader.\"\"\"\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up file handles.\"\"\"\n",
    "        for handle in self._file_handle_cache.values():\n",
    "            handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk Count: 4438 \n",
      "Input Tensor: torch.Size([2, 44100]) \n",
      "Label Tensor: torch.Size([2, 44100])\n",
      "Batch Input Tensor: torch.Size([1, 2, 44100]) \n",
      "Batch Label Tensor: torch.Size([1, 2, 44100])\n",
      "Loader Input Tensor: torch.Size([1, 2, 44100]) \n",
      "Loader Label Tensor: torch.Size([1, 2, 44100])\n",
      "\n",
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Direct Call Sanity Check\n",
    "input_audio, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "\n",
    "assert_f = (freq_orig // freq_scale) * chunk_duration\n",
    "\n",
    "# Expect shape: [2, 44100]\n",
    "assert input_audio.shape == (2, assert_f), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, assert_f), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch(1)\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_batch.shape == (1, 2, assert_f), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (1, 2, assert_f), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_collate.shape == (1, 2, assert_f), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (1, 2, assert_f), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "assert torch.equal(input_audio, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input_audio, label_audio = dataset[i]\n",
    "    assert input_audio.shape == (2, assert_f), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, assert_f), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, assert_f), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1, randomized=True)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=16, num_layers=4):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv1d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU()\n",
    "            ))\n",
    "        self.final_conv = nn.Conv1d(in_channels + num_layers * growth_rate, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            out = layer(torch.cat(features, dim=1))\n",
    "            features.append(out)\n",
    "        return self.final_conv(torch.cat(features, dim=1)) + x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Resizer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, target_size):\n",
    "        super(Resizer, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channel, out_channel, kernel_size=4, stride=2, padding=1)\n",
    "        self.target_size = target_size\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # Use interpolate for precise resizing\n",
    "        x = F.interpolate(x, size=self.target_size, mode='linear', align_corners=True)\n",
    "        return self.activation(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # Adjust x or skip size if they don't match\n",
    "        diff = x.size(2) - skip.size(2)\n",
    "        if diff > 0:\n",
    "            x = x[:, :, :skip.size(2)]\n",
    "        elif diff < 0:\n",
    "            x = nn.functional.pad(x, (0, -diff))\n",
    "        return x\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    # Simplified representation; adjust based on specific requirements or TasNet variant\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=8):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels) for _ in range(num_blocks)])\n",
    "        \n",
    "    def _build_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # Convert from (batch, channels, time) to (time, batch, channels)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.permute(1, 2, 0)  # Convert back to (batch, channels, time)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        # (batch, channels, time), e.g. (1, 2, 66150)\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.down2 = DownSample(2, 4)\n",
    "        self.down3 = DownSample(4, 8)\n",
    "        self.cv_tasnet = CV_TasNet_Block(8, 8)\n",
    "        self.temporal_attention = TemporalSelfAttention(8)\n",
    "\n",
    "        # Multi-scale feature fusion\n",
    "        # Fuse upsampled features and skip connection from downsampled features\n",
    "        self.up3 = UpSample(8, 4)\n",
    "        self.up2 = UpSample(4, 2)\n",
    "        self.resizer = Resizer(2, 2, assert_f)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Conv1d(4, 4, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(4, num_channels, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "                weight_norm(m, name='weight', dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip1 = self.down1(x)            # (batch, 2, (freq_orig // freq_scale) * chunk_duration)\n",
    "        skip2 = self.down2(skip1)        # (batch, 4, ((freq_orig // freq_scale) * chunk_duration) / 2)\n",
    "        skip3 = self.down3(skip2)        # (batch, 8, ((freq_orig // freq_scale) * chunk_duration) / 4)\n",
    "        x = self.cv_tasnet(skip3)        # (batch, 8, ((freq_orig // freq_scale) * chunk_duration) / 4)\n",
    "        x = self.temporal_attention(x)   # (batch, 8, ((freq_orig // freq_scale) * chunk_duration) / 4)\n",
    "        x = self.up3(x, skip3)           # (batch, 4, ((freq_orig // freq_scale) * chunk_duration) / 4)\n",
    "        x = self.up2(x, skip2)           # (batch, 2, ((freq_orig // freq_scale) * chunk_duration) / 2)\n",
    "        x = self.resizer(x)              # (batch, 2, (freq_orig // freq_scale) * chunk_duration)\n",
    "        x = torch.cat([x, skip1], dim=1) # (batch, 4, (freq_orig // freq_scale) * chunk_duration)\n",
    "        return self.up1(x)               # (batch, 2, (freq_orig // freq_scale) * chunk_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_splits(dataset, val_ratio=0.2, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_ratio * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    return SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "train_sampler, val_sampler = create_train_val_splits(dataset, val_ratio=0.2)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, n_fft=1024, hop_length=None, epsilon=1e-10):\n",
    "    window = torch.hann_window(n_fft).to(output.device)\n",
    "    assert output.dim() == 3 and target.dim() == 3, \"Input tensors must be 3D (batch, channels, time)\"\n",
    "    _, num_channels, _ = output.shape\n",
    "    loss = 0\n",
    "    for i in range(num_channels):\n",
    "        output_stft = torch.stft(output[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        target_stft = torch.stft(target[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        spectral_diff = torch.abs(torch.abs(output_stft) - torch.abs(target_stft) + epsilon)\n",
    "        loss += torch.mean(spectral_diff)\n",
    "    return loss / num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioUNet(num_channels=2).to(device).float() # Adjust audio_length and num_speakers\n",
    "criterion_mse = nn.MSELoss().to(device)              # Mean Squared Error\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scaler = torch.amp.GradScaler(enabled=(str(device) != 'cpu'), init_scale=2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 9,890\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1/10] | Mini-Batch [100/4438] | Train: 0.571767 | LR: 0.000500 | Effective Batch: 8\n",
      "Epoch [  1/10] | Mini-Batch [200/4438] | Train: 0.549238 | LR: 0.000500 | Effective Batch: 8\n",
      "Epoch [  1/10] | Mini-Batch [300/4438] | Train: 0.277730 | LR: 0.000500 | Effective Batch: 8\n",
      "Epoch [  1/10] | Mini-Batch [400/4438] | Train: 0.326018 | LR: 0.000500 | Effective Batch: 8\n",
      "Epoch [  1/10] | Mini-Batch [500/4438] | Train: 0.296549 | LR: 0.000500 | Effective Batch: 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accumulation_steps = 32      # Effective_batch_size = batch_size * accumulation_steps\n",
    "best_val_loss = float('inf') # Initialize as highest possible\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        with torch.amp.autocast(device_type=str(device)):\n",
    "            outputs = model(inputs)\n",
    "            mse_loss = criterion_mse(outputs, labels)\n",
    "            spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "            # Normalize mini losses to emulate larger batch size (not just accumulation)\n",
    "            loss = (mse_loss + spectral_weight * spec_loss) / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        # Showing the true mini-batch loss during logging\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "        if str(device) == \"cuda\":\n",
    "            del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch [{epoch+1:3}/{num_epochs}] | '\n",
    "                  f'Mini-Batch [{i+1:4}/{len(data_loader)}] | '\n",
    "                  f'Train: {(running_loss / 100):8.6f} | '\n",
    "                  f'LR: {optimizer.param_groups[-1][\"lr\"]:.6f} | '\n",
    "                  f'Effective Batch: {batch_size * accumulation_steps}')\n",
    "            running_loss = 0.0\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                outputs = model(inputs)\n",
    "                mse_loss = criterion_mse(outputs, labels)\n",
    "                spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "                loss = mse_loss + spectral_weight * spec_loss\n",
    "                val_loss += loss.item()\n",
    "            if device.type == \"cuda\":\n",
    "                del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch [{epoch+1:3}/{num_epochs}] | Validation: {avg_val_loss:8.6f}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, 'best_audio_unet.pth')\n",
    "    if str(device) == \"cuda\":\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
