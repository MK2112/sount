{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing 1 - Drums and Percussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.parametrizations import weight_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Allocated CUDA memory:   0.0000 GiB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(f\"Allocated CUDA memory: {torch.cuda.memory_allocated() / 1024 ** 3:8.4f} GiB\")\n",
    "\n",
    "num_epochs = 10\n",
    "data_dir = \"/mnt/data/Daftset/Dataset\"\n",
    "tmp_dir = \"/mnt/data/Daftset/Dataset_tmp\"\n",
    "batch_size = 1\n",
    "learning_rate = 1e-3\n",
    "num_channels = 2\n",
    "freq_orig = 44100\n",
    "freq_scale = 2\n",
    "chunk_duration = 2\n",
    "weight_decay = 1e-4\n",
    "spectral_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, tmp_dir: str, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        self.data_dir = data_dir      # Directory containing input *and* label files\n",
    "        self.tmp_dir = tmp_dir        # Directory for temporary files\n",
    "        self.input_tail = input_tail  # File extensions for input files\n",
    "        self.label_tail = label_tail  # File extensions for label files\n",
    "        self.chunk_size = chunk_duration * (freq_orig // freq_scale)  # Number of samples per chunk\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)  # List of tuples: (input_filename, label_filename)\n",
    "        self.input_label_lengths = self._load_audio_lengths(self.input_label_pairs)  # List of tuples: (input_length, label_length)\n",
    "        self.input_length = sum([in_len for in_len, _ in self.input_label_lengths])  # Total number of samples\n",
    "        self.batch_count = 0  # 'Global' counter for batch generation\n",
    "\n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> list:\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        file_tuples = []\n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _load_audio_lengths(self, file_tuples: list) -> list:\n",
    "        lengths = []\n",
    "        for in_fname, lb_fname in file_tuples:\n",
    "            audio_in = torchaudio.load(os.path.join(self.data_dir, in_fname))[0]\n",
    "            audio_lb = torchaudio.load(os.path.join(self.data_dir, lb_fname))[0]\n",
    "            # Trim both audios to equal length\n",
    "            max_length = max(audio_in.shape[1], audio_lb.shape[1])\n",
    "            if audio_in.shape[1] < max_length:\n",
    "                audio_in = torch.nn.functional.pad(audio_in, (0, max_length - audio_in.shape[1]))\n",
    "            elif audio_lb.shape[1] < max_length:\n",
    "                audio_lb = torch.nn.functional.pad(audio_lb, (0, max_length - audio_lb.shape[1]))\n",
    "            tmp_path_in = os.path.join(self.tmp_dir, in_fname)\n",
    "            tmp_path_lb = os.path.join(self.tmp_dir, lb_fname)\n",
    "            torchaudio.save(tmp_path_in, audio_in, freq_orig // freq_scale)\n",
    "            torchaudio.save(tmp_path_lb, audio_lb, freq_orig // freq_scale)\n",
    "            lengths.append((max_length, max_length))\n",
    "        return lengths\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.input_length // self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunk, label_chunk = [], []\n",
    "        remaining_samples = self.chunk_size\n",
    "        while remaining_samples > 0:\n",
    "            for (in_file, lb_file), (audio_length, _) in zip(self.input_label_pairs, self.input_label_lengths):\n",
    "                if global_start >= audio_length:\n",
    "                    global_start -= audio_length\n",
    "                    continue\n",
    "                samples_from_file = min(audio_length - global_start, remaining_samples)\n",
    "                path_in = os.path.join(self.tmp_dir, in_file)\n",
    "                path_lb = os.path.join(self.tmp_dir, lb_file)\n",
    "                audio_in = torchaudio.load(path_in)[0][:, global_start:global_start + samples_from_file]\n",
    "                audio_lb = torchaudio.load(path_lb)[0][:, global_start:global_start + samples_from_file]\n",
    "                input_chunk.append(audio_in)\n",
    "                label_chunk.append(audio_lb)\n",
    "                remaining_samples -= samples_from_file\n",
    "                global_start = 0  # Reset for next file\n",
    "                if remaining_samples == 0:\n",
    "                    break\n",
    "            if remaining_samples > 0:\n",
    "                # Start over if we've gone through all files and still need more samples\n",
    "                global_start = 0\n",
    "        return torch.cat(input_chunk, dim=1), torch.cat(label_chunk, dim=1)\n",
    "\n",
    "    def get_batch(self, batch_size, randomized=False):\n",
    "        if randomized:\n",
    "            max_start_idx = len(self) - 1\n",
    "            idx = np.random.randint(0, max_start_idx + 1)\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "        batch = []\n",
    "        for i in range(batch_size):\n",
    "            current_idx = (idx + i) % len(self) # Wrap around\n",
    "            batch.append(self.__getitem__(current_idx))\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk Count: 4491 \n",
      "Input Tensor: torch.Size([2, 44100]) \n",
      "Label Tensor: torch.Size([2, 44100])\n",
      "Batch Input Tensor: torch.Size([1, 2, 44100]) \n",
      "Batch Label Tensor: torch.Size([1, 2, 44100])\n",
      "Loader Input Tensor: torch.Size([1, 2, 44100]) \n",
      "Loader Label Tensor: torch.Size([1, 2, 44100])\n",
      "\n",
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Direct Call Sanity Check\n",
    "input_audio, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "\n",
    "assert_f = (freq_orig // freq_scale) * chunk_duration\n",
    "\n",
    "# Expect shape: [2, 44100]\n",
    "assert input_audio.shape == (2, assert_f), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, assert_f), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch(1)\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_batch.shape == (1, 2, assert_f), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (1, 2, assert_f), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_collate.shape == (1, 2, assert_f), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (1, 2, assert_f), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "assert torch.equal(input_audio, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input_audio, label_audio = dataset[i]\n",
    "    assert input_audio.shape == (2, assert_f), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, assert_f), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, assert_f), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1, randomized=True)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=16, num_layers=4):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv1d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU()\n",
    "            ))\n",
    "        self.final_conv = nn.Conv1d(in_channels + num_layers * growth_rate, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            out = layer(torch.cat(features, dim=1))\n",
    "            features.append(out)\n",
    "        return self.final_conv(torch.cat(features, dim=1)) + x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Resizer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, target_size):\n",
    "        super(Resizer, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channel, out_channel, kernel_size=4, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(target_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # Adjust x or skip size if they don't match\n",
    "        diff = x.size(2) - skip.size(2)\n",
    "        if diff > 0:\n",
    "            x = x[:, :, :skip.size(2)]\n",
    "        elif diff < 0:\n",
    "            x = nn.functional.pad(x, (0, -diff))\n",
    "        return x\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    # Simplified representation; adjust based on specific requirements or TasNet variant\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=8):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels) for _ in range(num_blocks)])\n",
    "        \n",
    "    def _build_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # Convert from (batch, channels, time) to (time, batch, channels)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.permute(1, 2, 0)  # Convert back to (batch, channels, time)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        # (batch, channels, time), e.g. (1, 2, 66150)\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.down2 = DownSample(2, 4)\n",
    "        self.down3 = DownSample(4, 8)\n",
    "        self.cv_tasnet = CV_TasNet_Block(8, 8)\n",
    "        self.temporal_attention = TemporalSelfAttention(8)\n",
    "\n",
    "        # Multi-scale feature fusion\n",
    "        # Fuse upsampled features and skip connection from downsampled features\n",
    "        self.up3 = UpSample(8, 4)\n",
    "        self.up2 = UpSample(4, 2)\n",
    "        self.resizer = Resizer(2, 2, assert_f)\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Conv1d(4, 4, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(4, num_channels, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "                weight_norm(m, name='weight')\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip1 = self.down1(x)            # (batch, 2, 66150)\n",
    "        skip2 = self.down2(skip1)        # (batch, 4, 33075)\n",
    "        skip3 = self.down3(skip2)        # (batch, 8, 16538)\n",
    "        x = self.cv_tasnet(skip3)        # (batch, 8, 16538)\n",
    "        x = self.temporal_attention(x)   # (batch, 8, 16538)\n",
    "        x = self.up3(x, skip3)           # (batch, 4, 16538)\n",
    "        x = self.up2(x, skip2)           # (batch, 2, 33075)\n",
    "        x = self.resizer(x)              # (batch, 2, 66150)\n",
    "        x = torch.cat([x, skip1], dim=1) # (batch, 4, 66150)\n",
    "        return self.up1(x)               # (batch, 2, 66150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir, tmp_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, n_fft=1024, hop_length=None):\n",
    "    assert output.dim() == 3 and target.dim() == 3, \"Input tensors must be 3D (batch, channels, time)\"\n",
    "    _, num_channels, _ = output.shape\n",
    "    loss = 0\n",
    "    for i in range(num_channels):\n",
    "        output_stft = torch.stft(output[:, i, :], n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
    "        target_stft = torch.stft(target[:, i, :], n_fft=n_fft, hop_length=hop_length, return_complex=True)\n",
    "        spectral_diff = torch.abs(torch.abs(output_stft) - torch.abs(target_stft))\n",
    "        loss += torch.mean(spectral_diff)\n",
    "    return loss / num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioUNet(num_channels=2).to(device) # Adjust audio_length and num_speakers\n",
    "criterion_mse = nn.MSELoss().to(device) # Mean Squared Error\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        mse_loss = criterion_mse(outputs, labels)\n",
    "        spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "        loss = mse_loss + spectral_weight * spec_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Unloading train example as early/thoroughly as possible\n",
    "        if device.type == \"cuda\":\n",
    "            del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if str(device) == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "    #if epoch % 2 == 0: # Validate every 2 epochs\n",
    "    #    model.eval()\n",
    "    #    with torch.no_grad():\n",
    "    #        val_loss = 0.0\n",
    "    #        for i, data in enumerate(val_loader, 0):\n",
    "    #            inputs, labels = data\n",
    "    #            inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #            outputs = model(inputs)\n",
    "    #            loss = criterion(outputs, labels)\n",
    "    #            val_loss += loss.item()\n",
    "    #        print(f'Validation loss: {val_loss / len(val_loader):.3f}')\n",
    "        \n",
    "    if epoch % 5 == 0: # Save every 5 epochs\n",
    "        torch.save(model.state_dict(), f'audio_unet_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
