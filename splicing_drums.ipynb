{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing - Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch_optimizer as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple\n",
    "from scipy.io.wavfile import write\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:\t\t\tcuda\n",
      "Allocated CUDA memory:\t  0.0000 GiB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:\\t\\t\\t{device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(f\"Allocated CUDA memory:\\t{torch.cuda.memory_allocated() / 1024 ** 3:8.4f} GiB\")\n",
    "\n",
    "num_epochs = 100        # Number of epochs to train\n",
    "data_dir = \"/mnt/data/Daftset/Dataset\"  # Directory containing the dataset (we will work on a copy)\n",
    "batch_size = 1          # 3060 tackles single-entry batches at max\n",
    "learning_rate = 1e-3    # Light learning rate\n",
    "num_channels = 2        # Number of audio channels\n",
    "freq_orig = 44100       # Original frequency of the audio files\n",
    "optim_k = 5             # Average weight updates over optim_k steps to stabilize training\n",
    "optim_alpha = 0.3       # Weight of influence of Lookahead's fast weights on the slow weights\n",
    "chunk_duration = 2      # Duration of training examples in seconds -> samples_per_example = (freq_orig) * chunk_duration\n",
    "weight_decay = 1e-4     # Weight decay for the optimizer\n",
    "spectral_weight = 0.6   # Spectral loss impact for total loss calculation\n",
    "accum_steps = 256       # Effective_batch_size = batch_size * accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller datasets may afford us to load the entire dataset at once during `init`, but this set is custom and sizewise unpredictable enough to require on-the-fly loading.\n",
    "Thing is, if we strictly load on request, how can we provide a `__len__` method for the dataset?<br>\n",
    "Thankfully, audio files have metadata we can use to determine their length without loading them, based on which we can provide a `__len__` method.\n",
    "\n",
    "- `_process_files` loads the in-out file pairs by name and as a list of name pairs, but not the actual audio data. We need that for ordered access to the dataset.\n",
    "- `_get_file_info` takes this list of name pairs and loads just the metadata of the audio files, which we can use to determine the length of all files. Note that we expect input and output to be the same size here. This is unrealistic, but we can trim or pad the labels accordingly later when we actually load the data.\n",
    "- The main data providing complexity is in `__get_item__`, where we \n",
    "    - calculate the idx of the requested input, label pair at a global scale (across all audio)\n",
    "    - assemble the input and the label pair, even across different files (load until we reach the requested sample size)\n",
    "    - trim or pad the label to match the input size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, batch_size: int = batch_size, chunk_duration: float = chunk_duration, \n",
    "                 freq_orig: int = freq_orig, input_tail: str='.wav', label_tail: str='_labeled.wav'):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_tail = input_tail\n",
    "        self.label_tail = label_tail\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.freq_orig = freq_orig\n",
    "        self.input_label_pairs = self._process_files(input_tail, label_tail)  # Get input-label file name pairs \n",
    "        self.file_info = self._get_file_info() # sift only through metadata for each file\n",
    "        self.input_length = sum(info['length'] for info in self.file_info) # Calculate total length and chunk information (from metadata)\n",
    "        self.chunk_size = int(chunk_duration * freq_orig)  # Ensure chunk_size is an integer\n",
    "        self.batch_count = 0\n",
    "        self._file_handle_cache = {} # Create cache for file handles\n",
    "\n",
    "    def _process_files(self, input_tail: str, label_tail: str) -> List[Tuple[str, str]]:\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        input_files = sorted([f for f in all_files if f.endswith(input_tail) and not f.endswith(label_tail)])\n",
    "        label_files_set = {f for f in all_files if f.endswith(label_tail)}\n",
    "        file_tuples = []\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            label_file = input_file.replace(input_tail, label_tail)\n",
    "            if label_file in label_files_set:\n",
    "                file_tuples.append((input_file, label_file))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing label file for {input_file}: Expected {label_file}.\")\n",
    "        \n",
    "        if not file_tuples:\n",
    "            raise ValueError(\"No matching input-label file pairs found.\")\n",
    "        return file_tuples\n",
    "\n",
    "    def _get_file_info(self) -> List[dict]:\n",
    "        file_info = []\n",
    "        for in_fname, lb_fname in self.input_label_pairs:\n",
    "            info = torchaudio.info(os.path.join(self.data_dir, in_fname))\n",
    "            length = info.num_frames\n",
    "            file_info.append({'length': length,\n",
    "                              'input_path': os.path.join(self.data_dir, in_fname),\n",
    "                              'label_path': os.path.join(self.data_dir, lb_fname)})\n",
    "        return file_info\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def _get_file_handle(self, file_path: str) -> sf.SoundFile:\n",
    "        if file_path not in self._file_handle_cache:\n",
    "            self._file_handle_cache[file_path] = sf.SoundFile(file_path, 'r')\n",
    "        return self._file_handle_cache[file_path]\n",
    "\n",
    "    def _normalize_audio(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        data = data.float()\n",
    "        if torch.abs(data).max() > 0:\n",
    "            data = data / (torch.abs(data).max() + 1e-8)\n",
    "        return torch.clamp(data, min=-0.99, max=0.99)  # Prevent extreme values\n",
    "\n",
    "    def _read_audio_chunk(self, file_path: str, start: int, length: int) -> torch.Tensor:\n",
    "        handle = self._get_file_handle(file_path)\n",
    "        handle.seek(start)\n",
    "        data = handle.read(length)\n",
    "        data = torch.from_numpy(data).T\n",
    "        return self._normalize_audio(data)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        total_chunks = self.input_length // self.chunk_size\n",
    "        return total_chunks // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        global_start = (idx * self.chunk_size) % self.input_length\n",
    "        input_chunks, label_chunks = [], []\n",
    "        samples_remaining = self.chunk_size\n",
    "        cumulative_length = 0\n",
    "        current_file_index = 0  # Track the current file being processed\n",
    "\n",
    "        while samples_remaining > 0:\n",
    "            if current_file_index >= len(self.file_info):\n",
    "                current_file_index = 0  # Wrap around to the first file\n",
    "\n",
    "            file_info = self.file_info[current_file_index]\n",
    "            \n",
    "            # Check if we are past the current file\n",
    "            if global_start >= cumulative_length + file_info['length']:\n",
    "                cumulative_length += file_info['length']\n",
    "                current_file_index += 1  # Move to the next file\n",
    "                continue\n",
    "            \n",
    "            local_start = global_start - cumulative_length\n",
    "            if local_start < 0:\n",
    "                local_start = 0\n",
    "\n",
    "            samples_from_file = min(file_info['length'] - local_start, samples_remaining)\n",
    "\n",
    "            if samples_from_file > 0:\n",
    "                input_chunk = self._read_audio_chunk(file_info['input_path'], local_start, samples_from_file)\n",
    "                label_chunk = self._read_audio_chunk(file_info['label_path'], local_start, samples_from_file)\n",
    "                input_chunks.append(input_chunk)\n",
    "                label_chunks.append(label_chunk)\n",
    "                samples_remaining -= samples_from_file\n",
    "            \n",
    "            cumulative_length += file_info['length']\n",
    "            current_file_index += 1  # Move to the next file after processing\n",
    "            \n",
    "        if samples_remaining > 0:\n",
    "            raise ValueError(\"Not enough samples available to fulfill the request.\")  # Handle as needed\n",
    "\n",
    "        return (torch.cat(input_chunks, dim=1), torch.cat(label_chunks, dim=1))\n",
    "\n",
    "    def get_batch(self, batch_size: int, randomized: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if randomized:\n",
    "            idx = np.random.randint(0, len(self))\n",
    "        else:\n",
    "            idx = batch_size * self.batch_count\n",
    "            if idx + batch_size > len(self):\n",
    "                self.batch_count = 0\n",
    "                idx = 0\n",
    "                \n",
    "        batch = [self.__getitem__((idx + i) % len(self)) for i in range(batch_size)]\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        \n",
    "        if not randomized:\n",
    "            self.batch_count += 1\n",
    "            \n",
    "        return batch_input, batch_label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input, batch_label = [torch.stack(items) for items in zip(*batch)]\n",
    "        batch_input = batch_input.view(batch_input.shape[0], 2, -1)\n",
    "        batch_label = batch_label.view(batch_label.shape[0], 2, -1)\n",
    "        return batch_input, batch_label\n",
    "\n",
    "    def __del__(self):\n",
    "        for handle in self._file_handle_cache.values():\n",
    "            handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=AudioDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk Count: 2345 \n",
      "Input Tensor: torch.Size([2, 88200]) \n",
      "Label Tensor: torch.Size([2, 88200])\n",
      "Batch Input Tensor: torch.Size([1, 2, 88200]) \n",
      "Batch Label Tensor: torch.Size([1, 2, 88200])\n",
      "Loader Input Tensor: torch.Size([1, 2, 88200]) \n",
      "Loader Label Tensor: torch.Size([1, 2, 88200])\n",
      "\n",
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Direct Call Sanity Check\n",
    "input_audio, label_audio = dataset[0]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "# Expect shape: [2, 44100]\n",
    "assert input_audio.shape == (2, assert_f), \"Error: Input tensor shape does not match expected size.\"\n",
    "assert label_audio.shape == (2, assert_f), \"Error: Label tensor shape does not match expected size.\"\n",
    "\n",
    "# Batch Call Sanity Check\n",
    "input_batch, label_batch = dataset.get_batch(1)\n",
    "print('Batch Input Tensor:', input_batch.shape, '\\nBatch Label Tensor:', label_batch.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_batch.shape == (1, 2, assert_f), \"Error: Batch input tensor shape does not match expected size.\"\n",
    "assert label_batch.shape == (1, 2, assert_f), \"Error: Batch label tensor shape does not match expected size.\"\n",
    "\n",
    "# Loader Call Sanity Check\n",
    "input_collate, label_collate = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', input_collate.shape, '\\nLoader Label Tensor:', label_collate.shape)\n",
    "\n",
    "# Expect shape: [1, 2, 44100]\n",
    "assert input_collate.shape == (1, 2, assert_f), \"Error: Loader input tensor shape does not match expected size.\"\n",
    "assert label_collate.shape == (1, 2, assert_f), \"Error: Loader label tensor shape does not match expected size.\"\n",
    "\n",
    "# Check if input_audio and input_collate are equal\n",
    "assert torch.equal(input_audio, input_collate.squeeze(0)), \"Error: Collate Loader vs. Direct Call are not equal.\"\n",
    "\n",
    "# Test multiple samples via direct call\n",
    "for i in range(1, 10):\n",
    "    input_audio, label_audio = dataset[i]\n",
    "    assert input_audio.shape == (2, assert_f), f\"Error at index {i}: Input tensor shape mismatch.\"\n",
    "    assert label_audio.shape == (2, assert_f), f\"Error at index {i}: Label tensor shape mismatch.\"\n",
    "\n",
    "# Test multiple batches via get_batch\n",
    "for _ in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at index {i}: Batch label tensor shape mismatch.\"\n",
    "\n",
    "# Check edge cases (last sample)\n",
    "input_audio_last, label_audio_last = dataset[len(dataset) - 1]\n",
    "assert input_audio_last.shape == (2, assert_f), \"Error: Last sample input tensor shape mismatch.\"\n",
    "assert label_audio_last.shape == (2, assert_f), \"Error: Last sample label tensor shape mismatch.\"\n",
    "\n",
    "# Check random access in get_batch\n",
    "for i in range(5):\n",
    "    input_batch, label_batch = dataset.get_batch(1, randomized=True)\n",
    "    assert input_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch input tensor shape mismatch.\"\n",
    "    assert label_batch.shape == (1, 2, assert_f), f\"Error at loop {i}: Randomized batch label tensor shape mismatch.\"\n",
    "\n",
    "# Taking a listen ensures consistent pair assembly\n",
    "input_audio, label_audio = dataset[np.random.randint(0, len(dataset)-1)]\n",
    "input_audio = input_audio.T.numpy()\n",
    "label_audio = label_audio.T.numpy()\n",
    "write('input_sample.wav', freq_orig, input_audio)\n",
    "write('label_sample.wav', freq_orig, label_audio)\n",
    "\n",
    "print(\"\\nSanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate=32, num_layers=4):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_channels + i * growth_rate\n",
    "            self.norms.append(nn.InstanceNorm1d(in_ch))\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv1d(in_ch, growth_rate, kernel_size=3, padding=1),\n",
    "                nn.InstanceNorm1d(growth_rate),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.1)))\n",
    "        final_ch = in_channels + num_layers * growth_rate\n",
    "        self.final_norm = nn.InstanceNorm1d(final_ch)\n",
    "        self.final_conv = nn.Conv1d(final_ch, in_channels, kernel_size=1)\n",
    "        self.output_norm = nn.InstanceNorm1d(in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for norm, layer in zip(self.norms, self.layers):\n",
    "            inputs = torch.cat(features, dim=1)\n",
    "            inputs = norm(inputs)\n",
    "            out = layer(inputs)\n",
    "            features.append(out)\n",
    "        out = self.final_norm(torch.cat(features, dim=1))\n",
    "        out = self.final_conv(out)\n",
    "        out = self.output_norm(out)\n",
    "        return (out * 0.5) + x\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Resizer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, target_size):\n",
    "        super(Resizer, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channel, out_channel, kernel_size=4, stride=2, padding=1)\n",
    "        self.target_size = target_size\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=self.target_size, mode='linear', align_corners=True)\n",
    "        return self.activation(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='linear', align_corners=True),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        diff = x.size(2) - skip.size(2)\n",
    "        if diff > 0:\n",
    "            x = x[:, :, :skip.size(2)]\n",
    "        elif diff < 0:\n",
    "            x = nn.functional.pad(x, (0, -diff))\n",
    "        return x\n",
    "\n",
    "class CV_TasNet_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=12):\n",
    "        super(CV_TasNet_Block, self).__init__()\n",
    "        self.blocks = nn.ModuleList([self._build_block(in_channels, out_channels) for _ in range(num_blocks)])\n",
    "        \n",
    "    def _build_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + residual\n",
    "            residual = x\n",
    "        return x\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x.permute(1, 2, 0)\n",
    "\n",
    "class AudioUNet(nn.Module):\n",
    "    def __init__(self, num_channels=2):\n",
    "        super(AudioUNet, self).__init__()\n",
    "        self.down1 = ResidualDenseBlock(num_channels)\n",
    "        self.down2 = DownSample(2, 4)\n",
    "        self.down3 = DownSample(4, 8)\n",
    "        self.down4 = DownSample(8, 16)\n",
    "        self.cv_tasnet = CV_TasNet_Block(16, 16)\n",
    "        self.temporal_attention = TemporalSelfAttention(16)\n",
    "        self.up4 = UpSample(16, 8)\n",
    "        self.up3 = UpSample(8, 4)\n",
    "        self.up2 = UpSample(4, 2)\n",
    "        self.resizer = Resizer(2, 2, assert_f)\n",
    "        self.up1 = nn.Sequential(nn.Conv1d(4, 4, kernel_size=3, padding=1),\n",
    "                                 nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(4, num_channels, kernel_size=3, padding=1),\n",
    "                                 nn.Tanh()) # Normalize to [-1, 1] for audio\n",
    " \n",
    "    def forward(self, x):\n",
    "        skip1 = self.down1(x)            # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        skip2 = self.down2(skip1)        # (batch, 4, ((freq_orig) * chunk_duration) / 2)\n",
    "        skip3 = self.down3(skip2)        # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        skip4 = self.down4(skip3)        # (batch, 16, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.cv_tasnet(skip4)        # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.temporal_attention(x)   # (batch, 16, ((freq_orig) * chunk_duration) / 8)\n",
    "        x = self.up4(x, skip4)           # (batch, 8, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up3(x, skip3)           # (batch, 4, ((freq_orig) * chunk_duration) / 4)\n",
    "        x = self.up2(x, skip2)           # (batch, 2, ((freq_orig) * chunk_duration) / 2)\n",
    "        x = self.resizer(x)              # (batch, 2, (freq_orig) * chunk_duration)\n",
    "        x = torch.cat([x, skip1], dim=1) # (batch, 4, (freq_orig) * chunk_duration)\n",
    "        return self.up1(x)               # (batch, 2, (freq_orig) * chunk_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_splits(dataset, val_ratio=0.2, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_ratio * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    return SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "train_sampler, val_sampler = create_train_val_splits(dataset, val_ratio=0.2, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=AudioDataset.collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(output, target, n_fft=1024, hop_length=None, epsilon=1e-8):\n",
    "    if hop_length is None:\n",
    "        hop_length = n_fft // 4\n",
    "    window = torch.hann_window(n_fft).to(output.device)\n",
    "    def safe_log(x):\n",
    "        return torch.log(torch.clamp(x, min=epsilon))\n",
    "    loss = 0\n",
    "    for i in range(output.shape[1]):\n",
    "        output_stft = torch.stft(output[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        target_stft = torch.stft(target[:, i, :], n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)\n",
    "        output_mag = torch.abs(output_stft)\n",
    "        target_mag = torch.abs(target_stft)\n",
    "        output_log_mag = safe_log(output_mag + epsilon)\n",
    "        target_log_mag = safe_log(target_mag + epsilon)\n",
    "        mag_loss = torch.mean(torch.abs(output_log_mag - target_log_mag))\n",
    "        output_phase = torch.angle(output_stft)\n",
    "        target_phase = torch.angle(target_stft)\n",
    "        phase_loss = 1 - torch.mean(torch.cos(output_phase - target_phase))\n",
    "        loss += mag_loss + 0.1 * phase_loss\n",
    "    return loss / output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 46,096\n"
     ]
    }
   ],
   "source": [
    "model = AudioUNet(num_channels=2).to(device).float() # Adjust audio_length and num_speakers\n",
    "model = torch.compile(model)\n",
    "criterion_mse = nn.MSELoss().to(device) # Mean Squared Error\n",
    "base_optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=1e-8)\n",
    "\n",
    "# Switching between providing 'fast weights' and 'slow weights' for AdamW optimizer update calculations\n",
    "optimizer = optim.Lookahead(base_optimizer, k=optim_k, alpha=optim_alpha)\n",
    "\n",
    "# Gradually warm and then cool down LR over time\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate*2, epochs=num_epochs, steps_per_epoch=len(train_loader) // accum_steps,\n",
    "                       pct_start=0.1, anneal_strategy='cos', div_factor=10.0, final_div_factor=1000.0)\n",
    "\n",
    "print(f\"Model Parameter Count: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Batch Size: 256 examples\n",
      "Epoch [  1/100] | Mini-Batch [ 512/2345] | Train: 2.584758 | LR: 0.000204\n",
      "Epoch [  1/100] | Mini-Batch [1024/2345] | Train: 2.547614 | LR: 0.000215\n",
      "Epoch [  1/100] | Mini-Batch [1536/2345] | Train: 2.465619 | LR: 0.000233\n",
      "Epoch [  1/100] | Validation: 1.910097\n",
      "Epoch [  2/100] | Mini-Batch [ 512/2345] | Train: 2.485328 | LR: 0.000275\n",
      "Epoch [  2/100] | Mini-Batch [1024/2345] | Train: 2.376441 | LR: 0.000311\n",
      "Epoch [  2/100] | Mini-Batch [1536/2345] | Train: 2.467648 | LR: 0.000353\n",
      "Epoch [  2/100] | Validation: 1.771295\n",
      "Epoch [  3/100] | Mini-Batch [ 512/2345] | Train: 2.314434 | LR: 0.000428\n",
      "Epoch [  3/100] | Mini-Batch [1024/2345] | Train: 2.346556 | LR: 0.000486\n",
      "Epoch [  3/100] | Mini-Batch [1536/2345] | Train: 2.236746 | LR: 0.000548\n",
      "Epoch [  3/100] | Validation: 1.752925\n",
      "Epoch [  4/100] | Mini-Batch [ 512/2345] | Train: 2.254073 | LR: 0.000650\n",
      "Epoch [  4/100] | Mini-Batch [1024/2345] | Train: 2.127245 | LR: 0.000723\n",
      "Epoch [  4/100] | Mini-Batch [1536/2345] | Train: 2.169895 | LR: 0.000799\n",
      "Epoch [  4/100] | Validation: 1.649621\n",
      "Epoch [  5/100] | Mini-Batch [ 512/2345] | Train: 2.063485 | LR: 0.000917\n",
      "Epoch [  5/100] | Mini-Batch [1024/2345] | Train: 2.070338 | LR: 0.000998\n",
      "Epoch [  5/100] | Mini-Batch [1536/2345] | Train: 2.012651 | LR: 0.001080\n",
      "Epoch [  5/100] | Validation: 1.596624\n",
      "Epoch [  6/100] | Mini-Batch [ 512/2345] | Train: 1.955225 | LR: 0.001202\n",
      "Epoch [  6/100] | Mini-Batch [1024/2345] | Train: 1.936250 | LR: 0.001283\n",
      "Epoch [  6/100] | Mini-Batch [1536/2345] | Train: 1.868482 | LR: 0.001362\n",
      "Epoch [  6/100] | Validation: 1.585174\n",
      "Epoch [  7/100] | Mini-Batch [ 512/2345] | Train: 1.868598 | LR: 0.001477\n",
      "Epoch [  7/100] | Mini-Batch [1024/2345] | Train: 1.815494 | LR: 0.001550\n",
      "Epoch [  7/100] | Mini-Batch [1536/2345] | Train: 1.819149 | LR: 0.001619\n",
      "Epoch [  7/100] | Validation: 1.547610\n",
      "Epoch [  8/100] | Mini-Batch [ 512/2345] | Train: 1.743362 | LR: 0.001714\n",
      "Epoch [  8/100] | Mini-Batch [1024/2345] | Train: 1.806034 | LR: 0.001772\n",
      "Epoch [  8/100] | Mini-Batch [1536/2345] | Train: 1.733938 | LR: 0.001823\n",
      "Epoch [  8/100] | Validation: 1.541083\n",
      "Epoch [  9/100] | Mini-Batch [ 512/2345] | Train: 1.736313 | LR: 0.001889\n",
      "Epoch [  9/100] | Mini-Batch [1024/2345] | Train: 1.704305 | LR: 0.001925\n",
      "Epoch [  9/100] | Mini-Batch [1536/2345] | Train: 1.678358 | LR: 0.001955\n",
      "Epoch [  9/100] | Validation: 1.460552\n",
      "Epoch [ 10/100] | Mini-Batch [ 512/2345] | Train: 1.643433 | LR: 0.001985\n",
      "Epoch [ 10/100] | Mini-Batch [1024/2345] | Train: 1.645359 | LR: 0.001996\n",
      "Epoch [ 10/100] | Mini-Batch [1536/2345] | Train: 1.618806 | LR: 0.002000\n",
      "Epoch [ 10/100] | Validation: 1.340214\n",
      "Epoch [ 11/100] | Mini-Batch [ 512/2345] | Train: 1.614009 | LR: 0.002000\n",
      "Epoch [ 11/100] | Mini-Batch [1024/2345] | Train: 1.594675 | LR: 0.002000\n",
      "Epoch [ 11/100] | Mini-Batch [1536/2345] | Train: 1.545130 | LR: 0.001999\n",
      "Epoch [ 11/100] | Validation: 1.346945\n",
      "Epoch [ 12/100] | Mini-Batch [ 512/2345] | Train: 1.569690 | LR: 0.001999\n",
      "Epoch [ 12/100] | Mini-Batch [1024/2345] | Train: 1.497696 | LR: 0.001998\n",
      "Epoch [ 12/100] | Mini-Batch [1536/2345] | Train: 1.548714 | LR: 0.001998\n",
      "Epoch [ 12/100] | Validation: 1.297548\n",
      "Epoch [ 13/100] | Mini-Batch [ 512/2345] | Train: 1.483627 | LR: 0.001996\n",
      "Epoch [ 13/100] | Mini-Batch [1024/2345] | Train: 1.517811 | LR: 0.001996\n",
      "Epoch [ 13/100] | Mini-Batch [1536/2345] | Train: 1.471322 | LR: 0.001995\n",
      "Epoch [ 13/100] | Validation: 1.292412\n",
      "Epoch [ 14/100] | Mini-Batch [ 512/2345] | Train: 1.469734 | LR: 0.001993\n",
      "Epoch [ 14/100] | Mini-Batch [1024/2345] | Train: 1.461117 | LR: 0.001992\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf') # Initialize as highest possible\n",
    "\n",
    "print(f'Effective Batch Size: {batch_size * accum_steps} examples')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        with torch.amp.autocast(device_type=str(device)):\n",
    "            outputs = model(inputs)\n",
    "            mse_loss = criterion_mse(outputs, labels)\n",
    "            spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "            # Normalize mini losses to emulate larger batch size (not just accumulation)\n",
    "            loss = (mse_loss + spectral_weight * spec_loss) / accum_steps\n",
    "        loss.backward() # Accumulate gradients\n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN detected in loss at epoch {epoch+1}, batch {i+1}\")\n",
    "            print(f\"MSE Loss: {mse_loss.item()}, Spectral Loss: {spec_loss.item()}\")\n",
    "        # Showing the true mini-batch loss during logging\n",
    "        running_loss += loss.item() * accum_steps\n",
    "        if str(device) == \"cuda\":\n",
    "            del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        if i % (2 * accum_steps) == (2 * accum_steps) - 1:\n",
    "            print(f'Epoch [{epoch+1:3}/{num_epochs}] | '\n",
    "                  f'Mini-Batch [{i+1:4}/{len(data_loader)}] | '\n",
    "                  f'Train: {(running_loss / 500):8.6f} | '\n",
    "                  f'LR: {optimizer.param_groups[-1][\"lr\"]:.6f}')\n",
    "            running_loss = 0.0\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            with torch.amp.autocast(device_type=str(device)):\n",
    "                outputs = model(inputs)\n",
    "                mse_loss = criterion_mse(outputs, labels)\n",
    "                spec_loss = spectral_loss(outputs, labels, n_fft=1024, hop_length=256)\n",
    "                loss = mse_loss + spectral_weight * spec_loss\n",
    "                val_loss += loss.item()\n",
    "            if device.type == \"cuda\":\n",
    "                del inputs, labels, outputs, mse_loss, spec_loss, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch [{epoch+1:3}/{num_epochs}] | Validation: {avg_val_loss:8.6f}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, 'best_audio_unet.pth')\n",
    "    if str(device) == \"cuda\":\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_f = (freq_orig) * chunk_duration\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "        state_dict = state_dict['model_state_dict']\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if 'weight_mask' not in key:\n",
    "            new_key = key.replace('weight_orig', 'weight')\n",
    "            new_state_dict[new_key] = value\n",
    "    model = AudioUNet(num_channels=2).to(device).float()\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_audio(audio_path, target_sample_rate=44100):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != target_sample_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = transform(waveform)\n",
    "    \n",
    "    # Normalize the waveform\n",
    "    waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "    \n",
    "    # Ensure the waveform has 2 channels (stereo)\n",
    "    if waveform.shape[0] == 1:\n",
    "        waveform = waveform.repeat(2, 1)\n",
    "    elif waveform.shape[0] > 2:\n",
    "        waveform = waveform[:2, :]\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def infer_audio(model, audio_path, output_path, device, chunk_duration=2, sample_rate=44100):\n",
    "    waveform = preprocess_audio(audio_path, target_sample_rate=sample_rate)\n",
    "    chunk_size = int(chunk_duration * sample_rate)\n",
    "    total_length = waveform.shape[1]\n",
    "    processed_waveform = torch.zeros_like(waveform)\n",
    "    overlap_count = torch.zeros(waveform.shape[1], dtype=torch.float32)\n",
    "    overlap = chunk_size // 2\n",
    "    window = torch.hann_window(chunk_size, dtype=torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, total_length, overlap):\n",
    "            end = min(start + chunk_size, total_length)\n",
    "            if end - start < chunk_size:\n",
    "                chunk = torch.zeros((2, chunk_size), dtype=torch.float32)\n",
    "                chunk[:, :(end - start)] = waveform[:, start:end]\n",
    "            else:\n",
    "                chunk = waveform[:, start:end]\n",
    "            chunk = chunk.unsqueeze(0).to(device)\n",
    "            output = model(chunk).squeeze(0).cpu()\n",
    "            if end - start < chunk_size:\n",
    "                valid_length = end - start\n",
    "                output = output[:, :valid_length]\n",
    "                window_section = window[:valid_length]\n",
    "            else:\n",
    "                window_section = window\n",
    "            output = output * window_section.view(1, -1)\n",
    "            if end - start < chunk_size:\n",
    "                processed_waveform[:, start:end] += output\n",
    "                overlap_count[start:end] += window_section\n",
    "            else:\n",
    "                processed_waveform[:, start:start + chunk_size] += output\n",
    "                overlap_count[start:start + chunk_size] += window_section\n",
    "    mask = overlap_count > 0\n",
    "    processed_waveform[:, mask] /= overlap_count[mask].view(1, -1)\n",
    "    processed_waveform = processed_waveform / (torch.max(torch.abs(processed_waveform)) + 1e-8)\n",
    "    processed_waveform = torch.clamp(processed_waveform, -0.99, 0.99)\n",
    "    sf.write(output_path, processed_waveform.T.numpy(), sample_rate)\n",
    "    print(f\"Generated audio saved at: {output_path}\")\n",
    "    return processed_waveform\n",
    "\n",
    "model_path  = \"best_audio_unet.pth\"\n",
    "audio_path  = \"input_audio.wav\"\n",
    "output_path = \"generated_audio.wav\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = load_model(model_path, device)\n",
    "infer_audio(model, audio_path, output_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
