{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Splicing 1 - Drums and Percussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(f\"Allocated CUDA memory: {torch.cuda.memory_allocated() / 1024 ** 3:6.3f} GiB\") # Should be 0.000 GiB\n",
    "\n",
    "num_epochs = 10\n",
    "data_dir = \"\"\n",
    "batch_size = 1\n",
    "learning_rate = 1e-3\n",
    "num_channels = 2\n",
    "freq_orig = 44100\n",
    "freq_scale = 2\n",
    "chunk_duration = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_tail='.wav', label_tail='_labeled.wav'):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_tail = input_tail\n",
    "        self.label_tail = label_tail\n",
    "        self.chunk_size = chunk_duration * (freq_orig // freq_scale)\n",
    "        self.batch_count = 0\n",
    "        self.input_files = [f for f in os.listdir(data_dir) if f.endswith(input_tail) and not f.endswith(label_tail)]\n",
    "        self.chunk_counts = self._calculate_chunk_counts()\n",
    "\n",
    "    def _calculate_chunk_counts(self):\n",
    "        chunk_counts = []\n",
    "        for input_filename in self.input_files:\n",
    "            input_path = os.path.join(self.data_dir, input_filename)\n",
    "            input_audio, _ = torchaudio.load(input_path)\n",
    "            num_chunks = int(np.ceil(input_audio.shape[1] / self.chunk_size))\n",
    "            chunk_counts.append(num_chunks)\n",
    "        return torch.tensor(chunk_counts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.chunk_counts.sum()\n",
    "    \n",
    "    def _get_file_and_chunk_idx(self, global_idx):\n",
    "        cumulative = 0\n",
    "        # This is why _calculate_chunk_counts() doesn't return sum directly:\n",
    "        for file_idx, num_chunks in enumerate(self.chunk_counts):\n",
    "            if cumulative + num_chunks > global_idx:\n",
    "                chunk_idx = global_idx - cumulative\n",
    "                return file_idx, chunk_idx\n",
    "            cumulative += num_chunks\n",
    "        raise IndexError(\"Index out of range\")\n",
    "    \n",
    "    def __getitem__(self, global_idx):\n",
    "        # Retrieve file containing chunk_idx\n",
    "        file_idx, chunk_idx = self._get_file_and_chunk_idx(global_idx)\n",
    "        input_filename = self.input_files[file_idx]\n",
    "        label_filename = input_filename.replace(self.input_tail, self.label_tail)\n",
    "        input_path = os.path.join(self.data_dir, input_filename)\n",
    "        label_path = os.path.join(self.data_dir, label_filename)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            raise FileNotFoundError(f\"Label file {label_path} does not exist.\")\n",
    "        \n",
    "        # Load and resample input and label audio\n",
    "        input_audio = torchaudio.transforms.Resample(orig_freq=freq_orig, new_freq=freq_orig // freq_scale)(torchaudio.load(input_path)[0])\n",
    "        label_audio = torchaudio.transforms.Resample(orig_freq=freq_orig, new_freq=freq_orig // freq_scale)(torchaudio.load(label_path)[0])\n",
    "\n",
    "        # Trim length to ensure both are of the same size\n",
    "        length = min(input_audio.shape[1], label_audio.shape[1])\n",
    "        input_audio, label_audio = input_audio[:, :length], label_audio[:, :length]\n",
    "\n",
    "        # Calculate chunk start and end positions\n",
    "        start = chunk_idx * self.chunk_size\n",
    "        end = min(start + self.chunk_size, length)\n",
    "\n",
    "        # Check if the chunk is valid (non-empty)\n",
    "        if start >= length:\n",
    "            # Return empty tensor if the start index exceeds the audio length\n",
    "            return torch.zeros((input_audio.shape[0], 0)), torch.zeros((label_audio.shape[0], 0))\n",
    "\n",
    "        # Ensure that the chunk size is not empty\n",
    "        if end <= start:\n",
    "            return torch.zeros((input_audio.shape[0], 0)), torch.zeros((label_audio.shape[0], 0))\n",
    "\n",
    "        # Return the sliced audio chunks\n",
    "        return input_audio[:, start:end], label_audio[:, start:end]\n",
    "\n",
    "\n",
    "    def get_batch(self, batch_size, randomized=False):\n",
    "        if randomized:\n",
    "            idx = np.random.choice(len(self), batch_size, replace=False)\n",
    "        else:\n",
    "            idx = np.arange(self.batch_count, self.batch_count + batch_size) % len(self)\n",
    "            self.batch_count += batch_size\n",
    "\n",
    "        input_audios, label_audios = zip(*[self[i] for i in idx])\n",
    "        return input_audios, label_audios\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        input_audio, label_audio = zip(*batch)\n",
    "        input_audio = torch.nn.utils.rnn.pad_sequence(input_audio, batch_first=True)\n",
    "        label_audio = torch.nn.utils.rnn.pad_sequence(label_audio, batch_first=True)\n",
    "        return input_audio, label_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=AudioDataset.collate_fn)\n",
    "input_audio, label_audio = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Call Sanity Check -> Expect [2, 66150], [2, 66150]\n",
    "print('Total Chunk Count:', len(dataset), '\\nInput Tensor:', input_audio.shape, '\\nLabel Tensor:', label_audio.shape)\n",
    "\n",
    "# Loader Call Sanity Check -> Expect [1, 2, 66150], [1, 2, 66150]\n",
    "# TODO: WAAAAAY TOO BUGGY, sometimes returns [1, 2, 66150], sometimes [1, 2, 0] out of nowhere\n",
    "inputs, targets = next(iter(data_loader))\n",
    "print('Loader Input Tensor:', inputs.shape, '\\nLoader Label Tensor:', targets.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
